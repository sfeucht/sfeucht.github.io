---
disableComments: true
toc: false
math: true
---

# Writing That is Still Not Meaningful
November 30, 2025

<span style="color:gray">
<i>What exactly are we doing when we read LLM-generated text?</i>
</span>

Recently, a [nonsense LLM-generated paper](https://x.com/micahgoldblum/status/1989088547777966512?s=20) kicked up some outrage in the community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and [later rejected for violating conference guidelines](https://x.com/iclr_conf/status/1989349884227715257?s=20), I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer's comments:

![I found this paper very difficult to read and comprehend. Beginning with the abstract—which conveys almost no meaningful insight to non-expert readers—the paper remains largely opaque throughout. It introduces numerous topics without proper context or explanation, ultimately leading to the unsubstantiated claim that a "verification framework" has been established.
There are two possible explanations for this lack of clarity: (1) The paper may be written in an extremely dense and narrow style, understandable only to experts working directly in this specific subarea (which I am not), or (2) The extensive use of LLM-assisted writing tools may have resulted in text that appears technically sophisticated but lacks genuine substance or coherence.](/review3_big.png)

The frustration here resonated with me a lot—probably because I'd been in a very similar situation before. 

In 2020, I close-read approximately 118 AI-generated documents on cannabis legalization (and an equal number of human-written ones). It was a painstaking task: I needed to classify the [lexical aspectual class](https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23722/1/scidok_final.pdf) of every clause, mark coherence relations between clauses, and rate the argumentation quality of each document. But there were two things that made this difficult. First, I didn't know which articles were human-written and which were AI-generated. Second, the AI-generated documents were extremely uncanny. Take this sentence, for example:

> <tt>If weed’s not really a public health issue and you're really happy about it, get an understanding about the ways in which it will be able to influence your behaviour.</tt>

Is this someone's Reddit comment, posted without a second thought? Or is it a semi-competent language model's attempt to imitate the surface form of an argument? I was never really sure. But the quality of my annotations depended on actually understanding what was being said here, so I spent a lot of time re-reading these kinds of sentences over and over, trying to grasp some kind of meaning from them. It felt like I was having a stroke, or like I was being gaslit by the text. But then, I'd read something like 

> <tt>BART police already have a "marijuana alley" where potential customers could find sprayers and pagers ready to use and find it where they're supposed to.</tt>

and breathe a sigh of relief. I'd know that this document is nonsense—that it was generated by GPT-2, a disembodied probabilistic model that cannot smoke weed and has never been to the Bay Area. Thus, I felt safe in assuming that there was "nothing there" for me to annotate. 

The following summer (2021), I wrote an [essay](https://sfeucht.github.io/rereading) about this experience for a class. It includes some cool examples of AI-generated nonsense that has the *shape* of a sensible argument, without the content. In that essay, I argued that the text generated by LLMs is odd in that it is not language *yet*—not until a human is able to read and extract meaning from that text. In this way, LLM-generated text is strangely beautiful. 

## Writing That's Not a Paper 
LLMs are a lot better now than they were in 2020: almost every sentence that comes out of a frontier model is not only structurally coherent, but sensical in the context of the sentences that came before it. You can almost always follow the flow of an LLM-generated article from beginning to end without much difficulty. So, does that mean that these documents are just... meaningful now? 

Maybe not. There's no fundamental difference between what GPT-2 does and what GPT-5 does (to our knowledge). If GPT-2 gave us *sentences* that looked correct but did not actually say anything, then maybe GPT-5 gives us entire *papers* that look coherent but don't actually make any sense. Here's a screenshot I took of the culprit paper's introduction as an example.

![Deploying large language models in sensitive settings creates a need for verifiable inference: proofs that outputs were computed correctly without revealing proprietary weights or private inputs. Zero-knowledge ML (ZKML) offers this, yet current systems struggle to scale to Transformer architectures.
Existing frameworks compile networks into polynomial constraint systems; prover cost is dominated by the number of constraints (roughly linear in parameter count). Cryptographic advances (e.g., lookups, sumcheck, commitments) accelerate the protocol layer, but they do not remove the model-level redundancy intrinsic to attention—leaving many constraints structurally unnecessary.
Key idea (GaugeZKP). Exploit attention's gauge symmetries. Many parameterizations implement the same function. We rewrite deployed weights into a canonical form (constructed per head; see §3.4) without changing the model. A one-time Proof of Gauge Equivalence (PoGE) binds deployed and canonical weights; thereafter, per-inference Proofs of Verifiable Inference (PoVI) run only on the canonical model. Because this optimization is upstream of the prover, it composes with protocol-level speedups.
Scope and guarantees. We certify exact functional equivalence (no approximation); privacy follows from the zk proof system. Attention remains quadratic in sequence length. Canonicalization relies on full-column-rank projections and numerically stable QR/SPD roots; fixed-point precision uses scale 2^16 (deterministic choices yield identical outputs across implementations).](/intro.png)

I know nothing about this area, so I'll leave critique of the "substance" of this paper to others (*if* that substance exists; more on that later). But what struck me was that this introduction has all of the surface-level indicators of being cogent and well-written: plain language, italicized key terms, and even a bolded paragraph header that points you to the **Key Idea.** Nonetheless, when I read it, I have no idea what problem the "authors" were trying to solve, or how their "key idea" actually helps to solve it. 

The way I feel reading this introduction is how I used to feel reading technical papers in undergrad, when I first started trying to get into ML research. Scanning over the text, it looks like something that should make a lot of sense to an expert somewhere, but no matter how many times I re-read any sentence, I am no closer to understanding what's going on. It could be because I don't know about ZKML, but I don't think this is true. If I read the introduction of [this real paper](https://arxiv.org/pdf/2404.16109) on zero-knowledge proofs for LLMs, it's like a breath of fresh air: I actually understand what ZKML is, why it's interesting and hard, and even get a rough sense of what the authors did (even though it would take lots of work for me to truly understand it). 

Unfortunately, the folks who had to review this paper were subjected to worse LLM-gaslighting than I ever had to experience in my annotator days. In 2020, when I re-re-read a clause like "potential customers could find sprayers and pagers ready to use and find it where they're supposed to," the escape hatch was always right there (I knew it could be AI). But here, not only was the nonsensity of the text much more subtle, but the possibility that this paper was AI-generated might not have even crossed the reviewers' minds. If I was in this situation, I might have felt that old insecurity from undergrad creeping in, a nagging feeling that the problem is *me*, maybe even that I should keep my head down and not object. I can see that being a factor for why this could get past so many reviewers.[^1] 

## The Conduit Metaphor

This specific frustration of re-re-reading sentences to no avail isn't new; I would guess that most people have experienced it with human-written text. I know that I can sometimes get "stuck" on sentences, reading them over and over like a broken record. For me, this mostly happens when reading technical writing, but I've also experienced it reading fiction, and even forum posts.

Reading is usually very easy for us, and it almost happens involuntarily. Think of the Stroop effect, where it's hard to *not* read the content of a word when it's flashed in front of you. When we read text that's written by others, we understand their thoughts and intentions quite quickly, almost as if there's a wire conducting their throughts straight into our brains. This might be why, at least in western English-speaking cultures, we conceptualize language as a conduit for meaning. This is known as the <span style="font-variant:small-caps;">Conduit Metaphor</span>, [first described](http://www.biolinguagem.com/ling_cog_cult/reddy_1979_conduit_metaphor.pdf) by linguist Michael J. Reddy in 1979, who argued that it forms the basis of how English speakers conceptualize communication and meaning.[^2]

Under the <span style="font-variant:small-caps;">Conduit Metaphor</span>, we think of ideas as objects in our minds, and communication as the task of "packaging" these ideas into words and sentences before sending them to others to be unpacked. Reddy uses linguistic evidence to argue that this is how English speakers conceptualize language: "try to *get your thoughts across* better," "*put* those ideas *in* some other paragraph," "the sentence was *filled* with emotion". If you study language, you've probably also come across this assumption stated explicitly, like in work on applying information theory to language, where language is conceptualized as a noisy channel through which we send meaningful messages.  

However, as Reddy points out in his original essay, this metaphor is misleading. Ideas are not objects that we can take from our minds and wire directly into other people's brains. One example that George Lakoff and Mark Johnson give in their 1980 book *Metaphors We Live By* is ____. 

Instead of thinking of reading in terms of the <span style="font-variant:small-caps;">Conduit Metaphor</span>, where a given text "contains" meaning that must be unpackaged by the reader, we could also think of reading as a process of *generating* meaning from some object that exists in the world. Under this conceptualization, the process of reading is kind of like making a recipe from a cookbook—every person making that recipe will execute the instructions differently, depending on their kitchen and the ingredients that they have access to in their own home, leading to very different outcomes. (The <span style="font-variant:small-caps;">Conduit Metaphor</span> would be akin to ordering UberEats.)

I like this conceptual metaphor for reading because it seems to resonate with other ways in which we use the word "read." When we read tea leaves, we are generating meaning from a random blob based on our own personal symbols and conceptual structures. You can *read* someone's face, or "read into" their actions, but those interpretations will always be in terms of your own disposition, your own hopes and anxieties. The act of reading is often interpretation, rather than extraction: when someone says, "that's my reading of it," they are being totally precise, because every person will read a given text slightly differently.

So what is happening when we find ourselves stuck re-re-reading the same sentence? These moments show us that reading is *generation*, not extraction. If we accept that there is never any meaning "contained" inside a text, then these moments are not *failures* to extract some true meaning lurking inside a work, but simply moments where some symbols on a page are not triggering any ideas in our minds. This could happen for any number of reasons: fatigue, failure on the author's part to phrase their thoughts clearly, or simply a lack of relevant conceptual structure on the part of the reader (like when you try to read technical writing on an unfamiliar topic). And of course, it could happen when trying to read an artefact generated by a probabilistic language model that's imitating the *form* of a paper without any intention to write something that actually would be helpful for other people. 

## Ants Meaningfully Fornicating In the Sand

Under this framing, if a piece of text is meaningful to a reader, it doesn't really matter *how* that text came into being: whether it was human-written, AI-generated, or [formed by some ants in the dirt](https://arxiv.org/pdf/2308.05576). This is starting to get into philosophical territory; since I don't have a background in philosophy, I'll just plainly state the issue that arises for me here.[^3]

This "ants in the dirt" example that I linked to is from Hilary Putnam's 1981 book, *Reason, Truth, and History.* I learned about it from Matthew Mandelkern and Tal Linzen's article, "Do Language Models' Words Refer?", who explain it very well:
> Suppose that, at a picnic, you observe ants wending through the sand in a surprising pattern, which closely resembles the English sentence "Peano proved that arithmetic is incomplete". At the same time, you get a text message from Luke, who is taking a logic class. He writes, "Peano proved that arithmetic is incomplete".
>
> Intuitively, the two cases are very different, despite involving physically similar
patterns. The ants’ patterns do not say anything; they just happen to have formed
patterns which resemble meaningful words. Of course, you can interpret the pattern,
just as you can interpret an eagle’s flight as an auspicious augur; but these are interpretations you overlay on a natural pattern, not meanings intrinsic to the patterns themselves. By contrast, Luke’s words mean something definite on their own (regardless of whether you or anyone else interprets them): namely, that Peano proved that arithmetic is incomplete. What Luke said is false: it was Gödel who proved incompleteness. But
Luke said something, whereas the ants didn’t say anything at all. In particular, he said
something false about Peano, which means that his (use of the) word ‘Peano’ managed
to refer to Peano.

Because of everything that we just talked about, I do not think that there is any real difference meaning-wise between ants accidentally spelling a sentence and a human intentionally typing one. If I read a sentence that causes me to have a particular thought, then I have made meaning from that sentence, and so the sentence is meaningful. Of course, the difference is that when a human sends me a text, they are hoping for me to have a particular thought. But the thought in my head will never be the exact same as what they were intending, and if some ants happen to spell out the same sentence, it will have the same effect. 

In other words, I think that we can view *all* reading as interpretation, like an augur. I don't think that meaning is ever "intrinsic to the patterns themselves."

## + What Now?
There are some infamous examples of fake papers making it into philosophy journals... 

- even well-written human text. it's not that we convert thoughts to words and words back to thoughts. it's not a symmetric process. you can have thoughts you don't write down, but more importantly, people can read anything out of your words that you weren't thinking of. is this a continuum? 

But... what *if* the thing that the reviewers read from the work was actually good? 

- fake paper examples that david mentioned. yeah like who cares if the author thought it was fake, if they did actually say something interesting 
- derrida example? 

In a post-LLM world, we are now flooded with "meaningless" slop TODO talk about "faith" and generating meanings... 

<!-- banging your head against a really difficult to understand paper. you can only do this if you have faith that the writer was a human who had something important they wanted to say.  -->


[^1]: Of course, the right thing to do in this situation would always be to ask for the paper to be reassigned, or review to the best of your ability with low confidence; if a paper is that hard to understand, it probably shouldn't get published anyway. But a willingness to call BS seems to be a combination of seniority and personality. 
[^2]: I learned about this idea from George Lakoff and Mark Johnson's book, [*Metaphors We Live By*](https://en.wikipedia.org/wiki/Metaphors_We_Live_By), published in 1980. Thank you to Si Wu for recommending this book to me! 
[^3]: If you have a philosophy background and are interested in talking about this, please reach out! This has been bothering me a lot. 