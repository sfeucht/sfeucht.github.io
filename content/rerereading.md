---
disableComments: true
toc: false
math: true
---

# Writing That is Language Now?
November 15, 2025

<span style="color:gray">
<i>A follow-up to <a href="https://sfeucht.github.io/rereading">"Writing That's Not Language (Yet)."</a></i>
</span>

*TODO tom: replace specifics in first paragraph with more generics, add interesting tagline*

Recently, a nonsense LLM-generated paper [landed in the top 17% of ICLR submissions](https://x.com/micahgoldblum/status/1989088547777966512?s=20), with two reviewers giving it a score of 8/10. Although it was flagged by a third reviewer and [later desk-rejected](https://x.com/iclr_conf/status/1989349884227715257?s=20), this "paper" kicked up a flurry of outrage among AI researchers. Here's part of the review that first called out the paper:

![I found this paper very difficult to read and comprehend. Beginning with the abstract—which conveys almost no meaningful insight to non-expert readers—the paper remains largely opaque throughout. It introduces numerous topics without proper context or explanation, ultimately leading to the unsubstantiated claim that a "verification framework" has been established.
There are two possible explanations for this lack of clarity: (1) The paper may be written in an extremely dense and narrow style, understandable only to experts working directly in this specific subarea (which I am not), or (2) The extensive use of LLM-assisted writing tools may have resulted in text that appears technically sophisticated but lacks genuine substance or coherence.](/review3_big.png)

The frustration here was palpable. As soon as I saw the phrase "*appears* technically sophisticated but lacks genuine substance," I realized that I'd already written about this exact feeling a few years ago.

## Writing That's Not Language
In 2020, I close-read approximately 118 AI-generated documents on cannabis legalization (and an equal number of human-written ones). It was a painstaking task: I needed to classify the [lexical aspectual class](https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23722/1/scidok_final.pdf) of every clause, mark coherence relations between clauses, and rate the argumentation quality of each document. But there were two things that made this difficult. First, I didn't know which articles were human-written and which were AI-generated. Second, the AI-generated documents were extremely uncanny. Take this sentence, for example:

> <tt>If weed’s not really a public health issue and you're really happy about it, get an understanding about the ways in which it will be able to influence your behaviour.</tt>

Is this someone's Reddit comment, posted without a second thought? Or is it a semi-competent language model's attempt to imitate the surface form of an argument? I was never really sure. But the quality of my annotations depended on actually understanding what was being said here, so I spent a lot of time re-reading these kinds of sentences over and over, trying to grasp some kind of meaning from them. It felt like I was having a stroke, or like I was being gaslit by the text. But then, I'd read something like 

> <tt>BART police already have a "marijuana alley" where potential customers could find sprayers and pagers ready to use and find it where they're supposed to.</tt>

and breathe a sigh of relief. I'd know that this document is nonsense—that it was generated by GPT-2, a disembodied probabilistic model that cannot smoke weed and has never been to the Bay Area. Thus, I felt safe in assuming that there was "nothing there" for me to annotate. 

The following summer (2021), I wrote an [essay](https://sfeucht.github.io/rereading) about this experience for a class. It includes some cool examples of AI-generated nonsense that has the *shape* of a sensible argument, without the content. In that essay, I argued that the text generated by LLMs is odd in that it is not language *yet*—not until a human is able to read and extract meaning from that text. In this way, LLM-generated text is strangely beautiful. 

## Writing That's Not a Paper 
LLMs are a lot better now than they were in 2020: almost every sentence that comes out of a frontier model is not only structurally coherent, but sensical in the context of the sentences that came before it. You can almost always follow the flow of an LLM-generated article from beginning to end without much difficulty. But there's no fundamental difference between GPT-2 and GPT-5 (to our knowledge). So, does that mean that these documents are just... meaningful now? 

I don't think so, and this recent ICLR situation illustrates why. If GPT-2 gave us sentences that looked correct but did not actually say anything, then GPT-5 gives us *entire papers* that look coherent but don't actually make any sense. Here's a screenshot I took of the culprit paper's introduction.

![Deploying large language models in sensitive settings creates a need for verifiable inference: proofs that outputs were computed correctly without revealing proprietary weights or private inputs. Zero-knowledge ML (ZKML) offers this, yet current systems struggle to scale to Transformer architectures.
Existing frameworks compile networks into polynomial constraint systems; prover cost is dominated by the number of constraints (roughly linear in parameter count). Cryptographic advances (e.g., lookups, sumcheck, commitments) accelerate the protocol layer, but they do not remove the model-level redundancy intrinsic to attention—leaving many constraints structurally unnecessary.
Key idea (GaugeZKP). Exploit attention's gauge symmetries. Many parameterizations implement the same function. We rewrite deployed weights into a canonical form (constructed per head; see §3.4) without changing the model. A one-time Proof of Gauge Equivalence (PoGE) binds deployed and canonical weights; thereafter, per-inference Proofs of Verifiable Inference (PoVI) run only on the canonical model. Because this optimization is upstream of the prover, it composes with protocol-level speedups.
Scope and guarantees. We certify exact functional equivalence (no approximation); privacy follows from the zk proof system. Attention remains quadratic in sequence length. Canonicalization relies on full-column-rank projections and numerically stable QR/SPD roots; fixed-point precision uses scale 2^16 (deterministic choices yield identical outputs across implementations).](/intro.png)

Now, I know nothing about cryptography, and so I'll leave critique of the "substance" of this paper to others who are more smart than me. But what struck me was that this introduction has all of the surface-level hallmarks of being cogent and well-written: plain language, italicized key terms, and even a bolded paragraph header that points you to the **Key Idea.** Nonetheless, when I read it, I have no idea what problem the "authors" were trying to solve, and how their "key idea" actually helps to solve it. 

The way I feel reading this introduction is exactly like how I *used* to feel reading technical papers in undergrad, when I first started trying to do ML research. It looks like something that should make a lot of sense to an expert somewhere, but no matter how many times I re-read a sentence, I am no closer to understanding what it's about. It could be because I don't know about ZKML, but I don't think this is true: if I read the introduction of [this real paper](https://arxiv.org/pdf/2404.16109) on zero-knowledge proofs for LLMs, it's like a breath of fresh air. I actually understand what ZKML is, why it's interesting and hard, and even get a rough sense of what the authors did (even though it would take lots of work for me to actually understand it). 

Unfortunately, the folks who had to review this paper were subjected to worse LLM-gaslighting than I ever experienced in my aforementioned annotator days. In 2020, when I re-re-read a clause like "potential customers could find sprayers and pagers ready to use and find it where they're supposed to," the escape hatch was right there (I knew it could be AI). But here, not only was the nonsensity of the text much more subtle, but the possibility that this paper was AI-generated might not have even crossed the reviewers' minds. If I was in this situation, I might have felt that old insecurity from undergrad creeping in, a nagging feeling that the problem is *me*, maybe even that I should keep my head down and not object. I can see that being a factor for why this could get past so many reviewers.[^1] 

## Bad Human Writing 

This specific frustration of re-re-reading is not new. 

This frustration and  has existed long before LLMs existed. 
(importantly, a human could also write things that don't have meaning. think of that first example sentence. )
banging your head against a really difficult to understand paper. you can only do this if you have faith that the writer was a human who had something important they wanted to say. 

- fake paper examples that david mentioned. yeah like who cares if the author thought it was fake, if they did actually say something interesting 
- can we only do this if we have faith that the writer is a human?

re-re-reading shows the cracks in what reading is as a whole. it's *generating* meaning from text, not unpacking it. we are always generating/creating, not unpacking. 


## Ants Fornicating Meaninglessly In the Sand
- ants fornicating meaninglessly in the sand: https://arxiv.org/pdf/2308.05576
- doesn't matter if the meaning is there in the first place, meaning comes from us getting it? 
- you don't need this entire externalist philosophy here. 
[I WOULD ARGUE THAT THIS IS WHAT READING IS!!!! "MY READING" OF A TEXT. art, philosophy, derrida, etc. just degree of difficulty?] metaphors we live by. 

- badly written human text: derrida, bad paper, bad review replies. show the actual paper!! compare to actual legit paper!!!!! 
- even well-written human text. it's not that we convert thoughts to words and words back to thoughts. it's not a symmetric process. you can have thoughts you don't write down, but more importantly, people can read anything out of your words that you weren't thinking of. is this a continuum? 

"language as a conduit" lakoff 
ellie's opinion piece look at "meaning" idea https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041

But... what *if* the thing that the reviewers read from the work was actually good? 


[^1]: Of course, the right thing to do in this situation would always be to ask for the paper to be reassigned, or review to the best of your ability with low confidence; if a paper is that hard to understand, it probably shouldn't get published anyway. And a willingness to call BS seems to be a combination of seniority and personality. 
