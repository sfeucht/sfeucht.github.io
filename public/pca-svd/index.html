<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Easy way to calculate PCA - Sheridan Feucht</title>

<meta name="description" content="Let&rsquo;s say you have a matrix $A$ of dimension (model_dim, n_samples), i.e. the columns correspond to points in a dataset. If we want to calculate PCA, we first center by subtracting the mean of the columns of $A$.
The classic PCA approach is to then get the covariance matrix $AA^T$ and take its eigendecomposition. From other notes we know that since the covariance matrix is symmetric, the eigendecomposition is equivalent to the SVD. So we have $U_{AA^T}\Sigma U_{AA^T}^T$ where my ugly subscripts are to be very clear that these are the singular vectors/eigenvectors of the covariance matrix; we can use the same matrix on both sides because it&rsquo;s symmetric and it&rsquo;s also an eigendecomposition.">





<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://localhost:1313/favicon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
    

    
        <link rel="stylesheet" href="http://localhost:1313/css/style.78de91526730265cde940d6082f8460bc8c2b62e4eddf9093c33d728050dcc87.css" integrity="sha256-eN6RUmcwJlzelA1ggvhGC8jCti5O3fkJPDPXKAUNzIc=">
    





    

    





    
    
    

    
        <script src="http://localhost:1313/js/script.32e751300d2d69e2cb56a98d2c9d964d54a53a8fb7281ccdbd80f055c88e8d86.js" type="text/javascript" charset="utf-8" integrity="sha256-MudRMA0taeLLVqmNLJ2WTVSlOo&#43;3KBzNvYDwVciOjYY="></script>
    







<meta property="og:url" content="http://localhost:1313/pca-svd/">
  <meta property="og:site_name" content="Sheridan Feucht">
  <meta property="og:title" content="Easy way to calculate PCA">
  <meta property="og:description" content="Let’s say you have a matrix $A$ of dimension (model_dim, n_samples), i.e. the columns correspond to points in a dataset. If we want to calculate PCA, we first center by subtracting the mean of the columns of $A$.
The classic PCA approach is to then get the covariance matrix $AA^T$ and take its eigendecomposition. From other notes we know that since the covariance matrix is symmetric, the eigendecomposition is equivalent to the SVD. So we have $U_{AA^T}\Sigma U_{AA^T}^T$ where my ugly subscripts are to be very clear that these are the singular vectors/eigenvectors of the covariance matrix; we can use the same matrix on both sides because it’s symmetric and it’s also an eigendecomposition.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Easy way to calculate PCA">
  <meta name="twitter:description" content="Let’s say you have a matrix $A$ of dimension (model_dim, n_samples), i.e. the columns correspond to points in a dataset. If we want to calculate PCA, we first center by subtracting the mean of the columns of $A$.
The classic PCA approach is to then get the covariance matrix $AA^T$ and take its eigendecomposition. From other notes we know that since the covariance matrix is symmetric, the eigendecomposition is equivalent to the SVD. So we have $U_{AA^T}\Sigma U_{AA^T}^T$ where my ugly subscripts are to be very clear that these are the singular vectors/eigenvectors of the covariance matrix; we can use the same matrix on both sides because it’s symmetric and it’s also an eigendecomposition.">



    
        <link rel="webmention" href="https://webmention.io/hugo-theme-anubis/webmention" />
        
            <link rel="pingback" href="https://webmention.io/hugo-theme-anubis/xmlrpc" />
        
    
    
        <link rel="webmention" href="https://sfeucht.github.io/webmentions/receive" />
    







    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header"> 
            
                <div class="header-top">
    <h1 class="site-title">
    <a href="/">Sheridan Feucht</a>
</h1>
    <ul class="social-icons">


    
        <li>
            <a href="/research" title="Research" rel="me">
                <span class="social-text">
    research
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/blog" title="Blog" rel="me">
                <span class="social-text">
    blog
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://twitter.com/sheridan_feucht" title="Twitter" rel="me">
            <span class="social-text">
    twitter
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://bsky.app/profile/sfeucht.bsky.social" title="Bluesky" rel="me">
                <span class="social-text">
    bluesky
</span>



            </a>
        </li>
    

    
        
        
        <li>
            <a href="https://github.com/sfeucht" title="Github" rel="me">
            <span class="social-text">
    github
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/sfeucht_cv.pdf" title="Cv" rel="me">
                <span class="social-text">
    cv
</span>



            </a>
        </li>
    




</ul>
</div>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



            
        </header>
        <main id="main" tabindex="-1"> 
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">Easy way to calculate PCA</h1>

                
            </header>
        </div>
        






        <div class="content e-content">
            <p>Let&rsquo;s say you have a matrix $A$ of dimension (model_dim, n_samples), i.e. the columns correspond to points in a dataset. If we want to calculate PCA, we first center by subtracting the mean of the columns of $A$.</p>
<p>The classic PCA approach is to then get the covariance matrix $AA^T$ and take its eigendecomposition. From <a href="/Covariance_and_Whitening_Self_Explanation.pdf">other notes</a> we know that since the covariance matrix is symmetric, the eigendecomposition is equivalent to the SVD. So we have $U_{AA^T}\Sigma U_{AA^T}^T$ where my ugly subscripts are to be very clear that these are the singular vectors/eigenvectors of the covariance matrix; we can use the same matrix on both sides because it&rsquo;s symmetric and it&rsquo;s also an eigendecomposition.</p>
<p>We don&rsquo;t actually have to calculate $AA^T$. We can directly take the SVD of $A$ instead as $U_A\Sigma V_A^T$. Then, we know that $AA^T=(U_A\Sigma V_A^T)(V_A\Sigma U_A^T)=U_A\Sigma^2U_A^T$ and that the left singular vectors of the data matrix $A$ are the same as the eigenvectors of $A$&rsquo;s covariance matrix.</p>
<p>If you ever get confused about rows and columns, remember that you&rsquo;d always want to take the singular vectors that are the model dimension; in this case they are the left singular vectors, since $A$ is (model_dim, n_samples). Also, the covariance matrix has to be (model_dim, model_dim).</p>
<h1 id="two-ways-to-project-onto-subspace" >Two ways to &ldquo;project onto subspace&rdquo;
<span>
    <a href="#two-ways-to-project-onto-subspace">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h1><p>If the columns of $A\in\mathbb{R}^{(d,n)}$ are all orthogonal vectors, then $AA^T$ is also the matrix that projects activations onto the $n$-dimensional subspace spanned by columns of $A$. (If they were not orthogonal vectors, the projection matrix would be $A(A^TA)^{-1}A^T$.) I can&rsquo;t believe I never noticed this until now, need to think about deeper connections here.</p>
<p>There&rsquo;s two ways to &ldquo;project onto a subspace.&rdquo; There&rsquo;s the linear algebra 101 thing, where a matrix multiplication is a change of basis: so if I have a vector $v\in\mathbb{R}^d$ and multiply $A^Tv$, then that takes dot product of $v$ with each column of $A$, telling me &ldquo;how much&rdquo; that vector is in the direction of each of those columns (e.g. [0.2, 3.1]). We can reconstruct $v$ by just multiplying those coefficients by the columns of $A$ again: $A(A^Tv)$. But wait, we&rsquo;ve actually lost information now about anything that <em>can&rsquo;t</em> be spanned by the columns of $A$. That&rsquo;s why $AA^T$ works as a projection matrix, if you want to <em>only</em> get the information in $v$ that&rsquo;s in the subspace spanned by $A$&rsquo;s columns, but keep that information in $d$ dimensions.</p>
<h1 id="interpreting-the-svd-of-a-data-matrix-x" >Interpreting the SVD of a Data Matrix X
<span>
    <a href="#interpreting-the-svd-of-a-data-matrix-x">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h1><p>This is restating half of the stuff above, but looking at the other set of singular vectors. Say you have a data matrix $X\in\mathbb{R}^{n\times d}$, where each of the $n$ rows is a data point (e.g. a hidden state) of $d$ dimensions (e.g., model dimension). Assuming that $X$ is centered, then if you take the SVD $X=U\Sigma V^T$, its $d$-dimensional singular vectors $V^T$ are also the principal components of the data, as we talked about above.</p>
<p>But then, how to interpret the $n$-dimensional singular vectors $U$? Well, since the rows of $X$ are all data points, each row of $U\Sigma$ corresponds to a data point in $X$. Specifically, each of those rows gives you that data point expressed in the principal component basis given by $V^T$ (i.e., the coefficients on $V^T$&rsquo;s rows you need to construct that data point). So if you want to take the PCA-reduced version of the data in $X$, all you have to do is take the top-$k$ columns of $U\Sigma$, which automatically gives you a $n\times k$ matrix of $k$-dimensional vectors for every data point. This is super useful in practice; it&rsquo;s an extremely quick way to project data onto the first $k$ principal dimensions.</p>

        </div>
        

    



<div class="post-info">
    

    <a class="post-hidden-url u-url" href="http://localhost:1313/pca-svd/">http://localhost:1313/pca-svd/</a>
    <a href="http://localhost:1313/" class="p-name p-author post-hidden-author h-card" rel="me">Sheridan Feucht</a>


    <div class="post-taxonomies">
        
        
        
    </div>
</div>

    </article>

    
        
        
    

    

    

        </main>
        
            <footer class="common-footer">
    
    

    <div class="common-footer-bottom">
        
        <div class="copyright">
            <p>© Sheridan Feucht, 2026<br>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.<br>
            
            </p>  
        </div> 

        

    





<script>
const STORAGE_KEY = 'user-color-scheme'
const defaultTheme = "auto-without-switcher"

let currentTheme
let switchButton
let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

const autoChangeScheme = e => {
    currentTheme = e.matches ? 'dark' : 'light'
    document.documentElement.setAttribute('data-theme', currentTheme)
    changeButtonText()
}

document.addEventListener('DOMContentLoaded', function() {
    switchButton = document.querySelector('.theme-switcher')
    currentTheme = detectCurrentScheme()
    if (currentTheme == 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark')
    }
    if (currentTheme == 'auto') {
        autoChangeScheme(autoDefinedScheme);
        autoDefinedScheme.addListener(autoChangeScheme);
    }

    if (switchButton) {
        changeButtonText()
        switchButton.addEventListener('click', switchTheme, false)
    }
  
    showContent()
})

function detectCurrentScheme() {
    if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
        return localStorage.getItem(STORAGE_KEY)
    } 
    if (defaultTheme) {
        return defaultTheme
    } 
    if (!window.matchMedia) {
        return 'light'
    } 
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        return 'dark'
    }
    return 'light'
}

function changeButtonText()
{   
    if (switchButton) {
        switchButton.textContent = currentTheme == 'dark' ?  "❂" : "☾"
    }
}

function switchTheme(e) {
    if (currentTheme == 'dark') {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'light')
        document.documentElement.setAttribute('data-theme', 'light')
        currentTheme = 'light'
    } else {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'dark')
        document.documentElement.setAttribute('data-theme', 'dark')
        currentTheme = 'dark'
    }
    changeButtonText()
}

function showContent() {
    document.body.style.visibility = 'visible';
    document.body.style.opacity = 1;
}
</script>

   
    </div>

    <p class="h-card vcard">

    <a href=http://localhost:1313/ class="p-name u-url url fn" rel="me">Sheridan Feucht</a> 

     
        /
        <a class="p-email u-email email" rel="me" href="mailto:feucht.s@northeastern.edu">feucht.s@northeastern.edu</a>
    

     
        <img class="u-photo" src="/images/me.png" />
    
</p> 
</footer>

        
    </div>
</body>
</html>
