<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Notes on Geometry Papers - Sheridan Feucht</title>

<meta name="description" content="Not All LM Features are 1D Linear (Engels et al., 2024)

    
        
    

[Paper, Video] - They&rsquo;re basically trying to figure out how to decompose hidden states into sums of different functions of the input (features). So for example, you might have a 1-dimensional representation of an integer value 3, added to an n-dimensional representation of the language semantics of &ldquo;three,&rdquo; etc.">





<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://localhost:1313/favicon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
    

    
        <link rel="stylesheet" href="http://localhost:1313/css/style.78de91526730265cde940d6082f8460bc8c2b62e4eddf9093c33d728050dcc87.css" integrity="sha256-eN6RUmcwJlzelA1ggvhGC8jCti5O3fkJPDPXKAUNzIc=">
    





    

    





    
    
    

    
        <script src="http://localhost:1313/js/script.32e751300d2d69e2cb56a98d2c9d964d54a53a8fb7281ccdbd80f055c88e8d86.js" type="text/javascript" charset="utf-8" integrity="sha256-MudRMA0taeLLVqmNLJ2WTVSlOo&#43;3KBzNvYDwVciOjYY="></script>
    







<meta property="og:url" content="http://localhost:1313/geometry/">
  <meta property="og:site_name" content="Sheridan Feucht">
  <meta property="og:title" content="Notes on Geometry Papers">
  <meta property="og:description" content="Not All LM Features are 1D Linear (Engels et al., 2024) [Paper, Video] - They’re basically trying to figure out how to decompose hidden states into sums of different functions of the input (features). So for example, you might have a 1-dimensional representation of an integer value 3, added to an n-dimensional representation of the language semantics of “three,” etc.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Notes on Geometry Papers">
  <meta name="twitter:description" content="Not All LM Features are 1D Linear (Engels et al., 2024) [Paper, Video] - They’re basically trying to figure out how to decompose hidden states into sums of different functions of the input (features). So for example, you might have a 1-dimensional representation of an integer value 3, added to an n-dimensional representation of the language semantics of “three,” etc.">



    
        <link rel="webmention" href="https://webmention.io/hugo-theme-anubis/webmention" />
        
            <link rel="pingback" href="https://webmention.io/hugo-theme-anubis/xmlrpc" />
        
    
    
        <link rel="webmention" href="https://sfeucht.github.io/webmentions/receive" />
    







    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header"> 
            
                <div class="header-top">
    <h1 class="site-title">
    <a href="/">Sheridan Feucht</a>
</h1>
    <ul class="social-icons">


    
        <li>
            <a href="/research" title="Research" rel="me">
                <span class="social-text">
    research
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/blog" title="Blog" rel="me">
                <span class="social-text">
    blog
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://twitter.com/sheridan_feucht" title="Twitter" rel="me">
            <span class="social-text">
    twitter
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://bsky.app/profile/sfeucht.bsky.social" title="Bluesky" rel="me">
                <span class="social-text">
    bluesky
</span>



            </a>
        </li>
    

    
        
        
        <li>
            <a href="https://github.com/sfeucht" title="Github" rel="me">
            <span class="social-text">
    github
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/sfeucht_cv.pdf" title="Cv" rel="me">
                <span class="social-text">
    cv
</span>



            </a>
        </li>
    




</ul>
</div>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



            
        </header>
        <main id="main" tabindex="-1"> 
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">Notes on Geometry Papers</h1>

                
            </header>
        </div>
        






        <div class="content e-content">
            <h1 id="not-all-lm-features-are-1d-linear-engels-et-al-2024" >Not All LM Features are 1D Linear (Engels et al., 2024)
<span>
    <a href="#not-all-lm-features-are-1d-linear-engels-et-al-2024">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h1><p>[<a href="https://arxiv.org/pdf/2405.14860">Paper</a>, <a href="https://www.youtube.com/watch?v=eVlGeHA2Cnw&amp;t=858s">Video</a>] - They&rsquo;re basically trying to figure out how to decompose hidden states into sums of different functions of the input (features). So for example, you might have a 1-dimensional representation of an integer value 3, added to an n-dimensional representation of the language semantics of &ldquo;three,&rdquo; etc.</p>
<p>When you&rsquo;re doing this, there&rsquo;s a problem: how do you know whether a multi-dimensional feature you&rsquo;ve found is &ldquo;atomic&rdquo;, or whether it&rsquo;s actually possible to further decompose it into other sub-features?</p>
<h2 id="reducibility" >Reducibility
<span>
    <a href="#reducibility">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>To figure this out, they formally define <em>reducible</em> features—which crucially we don&rsquo;t actually want to count as a good/valid multi-dimensional feature. So for a given multi-dimensional feature $\bf f$ (defined as a function that maps from inputs to some $d_f$-dimensional subspace), can it actually be &ldquo;broken down&rdquo; (e.g. latitude and longitude)?</p>
<p>They come up with two metrics to see if this is approximately true:</p>
<ol>
<li>$S(f)$ - <strong>Separability index.</strong> For all possible ways to break down a feature into $\bf a$ and $\bf b$, what is the minimum mutual information $I(\textbf{a}, \textbf{b})$? For example, a multidimensional Gaussian will not actually have mutual information between subfeatures if you choose the right axes. Higher = harder to separate.</li>
<li>$M_\epsilon(f)$ - <strong>$\epsilon$-mixture index.</strong> Across all the token positions, can we pick a special vector $\bf v$ for which the feature can be projected near zero pretty often? (in practice, this is something like less than $\epsilon$ * standard deviation rather than zero). If you can find a vector $\bf v$ and offset $c$ so that $\textbf{v} \cdot \textbf{f(t)} + c$ is small for lots of token positions, then that means that in all of those cases that direction in the feature space wasn&rsquo;t really being used. Of course, $\bf v$ has to be within the subspace $\mathbb{R}^{d_f}$, because otherwise you could easily find a vector that&rsquo;s orthogonal to $\mathbb{R}^{d_f}$ and zeros out everything. Higher = easier to separate.</li>
</ol>
 <details>
<summary>Concrete-ish example of mixture index</summary>
Let's say your multi-dimensional feature $\bf f$ was dimension $d_f=2$ but to your dismay it was actually a mixture of two features, $\bf e_1$ and $\bf e_2$. For 60% of the token positions where the feature is active, it's using mostly $\bf e_1$, and for the other 40% it's using mostly $\bf e_2$. In such a case, you could choose $\bf v=e_2$ and get a zero dot product for 60% of tokens and one dot product for the other 40%, resulting in a score of around 0.60. This is pretty high, and it indicates that this so-called "multi-dimensional feature" is actually just a combination of sub-features represented by $\bf e_1$ and $\bf e_2$.
</details>
<p>In practice, $S(f)$ scores for GPT-2 features are mostly less than 0.2 (aka, most features are pretty separable), and $M_\epsilon(f)$ features are quite high, mostly bigger than 0.4 (aka, most features are pretty much mixtures). They use these scores to argue that their day-of-week features, since they have high $S(f)$ and low $M_\epsilon(f)$, are irreducible multi-dimensional features.</p>
<h2 id="going-from-sae-to-multi-dim-features" >Going from SAE to Multi-Dim Features
<span>
    <a href="#going-from-sae-to-multi-dim-features">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>If $\bf D$ is the decoder matrix of the SAE containing all of the output features, then to cluster features together, they take the cosine similarity between all decoder vectors and prune away cosine similarities below a threshold $T$. By construction these clusters will be &ldquo;$T$-orthogonal&rdquo; (i.e. if $T=0$ then they&rsquo;re truly orthogonal).</p>
<p>If the SAE is good at reconstructing, then when a multi-dimensional non-reducible feature $\bf f$ is active, <strong>they claim that one of these cosine-similar clusters will be equal to f.</strong> This is not super obvious. Naively, you&rsquo;d think it&rsquo;d be the opposite—that if $\bf f$ was a 2D feature that was properly reconstructed by the SAE, the SAE would have learned two orthogonal decoder vectors that span the space (and those would have close to zero cosine similarity). But if $\bf f$ is truly a multi-dimensional feature, it would never make sense to separate out these two features; then the SAE would get penalized by always having both features active whenever it needs to reconstruct $\bf f$. So instead, they guess that the SAE would learn a bunch of cosine-similar things that are all in $\bf f$ (intuitively, maybe this would be like learning separate &ldquo;Monday&rdquo;, &ldquo;Tuesday&rdquo;, &ldquo;Wednesday&rdquo; features).</p>
<p>So what they do is:</p>
<ol>
<li>Cluster all the SAE decoder vectors using this thresholding thing</li>
<li>To analyze a cluster, get cluster-specific reconstructions of all hidden states $\textbf{x}_{i,l}$.</li>
<li>Analyze that cluster&rsquo;s representations across a bunch of hidden states by looking at PCA projections, or running $S(f)$ and $M_\epsilon(f)$</li>
</ol>
<p>Two important details:</p>
<ul>
<li>In order to calculate these scores, they actually <em>first</em> calculate the PCA directions over the reconstructed activations, and then project onto PCA components 1-2, 2-3, 3-4, 4-5, averaging the separability/mixture over each of these planes.</li>
<li>This process implicitly filters for datapoints that are relevant for this cluster/feature. In step (2), if none of the cluster&rsquo;s features are active for a particular hidden state $\textbf{x}_{i,l}$, it is not included in the analysis. In the case of a days-of-the-week feature, the PCAs will of course look pretty nice, since the biggest variation between a bunch of weekday vectors will almost surely be information about weekday.</li>
</ul>
<p>Another cool thing is that they show that these features are actually cones, where the first principal component seems to be intensity of weekday-ness, perhaps.</p>
<p><img src="/geometry/engels_pca.png" alt=""></p>
<h2 id="intervening-on-circles" >Intervening on Circles
<span>
    <a href="#intervening-on-circles">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>While they do try intervening on the subspace found by the SAE, they get slightly better results by training probes to find good subspaces at each layer. Specifically, they</p>
<ol>
<li>Take the top-$k$ principal component directions of hidden states at the &ldquo;Monday&rdquo; position (pretty sure this position) across all the prompts and project onto there first.</li>
<li>Then train a linear probe $\bf P$ in that space that maps from $k$ to 2 dimensions, trained to correspond to <tt>circle</tt>($\alpha$) &ndash; e.g. for weekdays <tt>circle</tt>$(\alpha)=[\cos(2\pi\alpha/7), \sin(2\pi\alpha/7)]$.</li>
<li>To intervene on &ldquo;Monday&rdquo; and make it look like &ldquo;Wednesday&rdquo;, they just discard &ldquo;Monday.&rdquo; I am confused about how exactly this intervention works, because their Equation 6 seems wrong (you can&rsquo;t calculate <tt>circle</tt>$(\alpha_{j&rsquo;}) - \overline{x_{i,l}}$ directly because they are different dimensions). Equation 7 doesn&rsquo;t clarify, it seems to have a missing parenthesis.</li>
</ol>
<!-- Start with mean over prompts $\overline{x_{i,l}}$. Project that onto the circle with $\textbf{PW}_{i,l}$ and then calculate offset from <tt>circle</tt>(Wednesday). Then project that offset back to model space with $\textbf{W}^T\_{i,l}\textbf{P}^+$ and add back to the mean to get final activation.  -->
<p>Josh Engels mentions in the talk that the intervention only works if you ablate out everything else that&rsquo;s not in the PCA; I&rsquo;m not exactly sure what he means here, probably the fact that they have to mean-ablate everything by using $\overline{x_{i,l}}$ instead of the base activation.</p>
<h2 id="generic-ideastakeaways" >Generic Ideas/Takeaways
<span>
    <a href="#generic-ideastakeaways">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><ul>
<li>How does the model actually use these circular representations to calculate the answer?</li>
<li>Some of these facts must be piecewise memorized as well (e.g. Saturday is 1 day after Friday is almost surely memorized) &ndash; probably why they have to ablate all other weekday information.</li>
<li>Presumably lots of weekday embedding information is unrelated to circles &ndash; if you rotated within the circle subspace for tasks like &ldquo;What holidays are typically on [Thursday]?&rdquo; or &ldquo;What letter does [Thursday] start with?&rdquo; then you&rsquo;d probably see it doesn&rsquo;t affect anything.</li>
<li>It still worries me to think about multiple circles happening at once. If you have six orthogonal 2D circles, could that just be some alternate formulation of an n-dimensional subspace?</li>
</ul>
<h1 id="when-models-manipulate-manifolds--linebreaks-gurnee-et-al-2025" >When Models Manipulate Manifolds / Linebreaks (Gurnee et al., 2025)
<span>
    <a href="#when-models-manipulate-manifolds--linebreaks-gurnee-et-al-2025">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h1><p>They find 1-dimensional feature manifolds embedded in low-dimensional subspaces for: num. characters in a token, num. characters in current line, overall line width constraint, num characters <em>remaining</em> in the current line.</p>
<ul>
<li>It wouldn&rsquo;t really make sense to dedicate an entire 1D subspace to num characters in current line, because then you&rsquo;re wasting that entire dimension if you&rsquo;re in a doc that doesn&rsquo;t have linebreaks. This is really strong evidence for superposition; as they note, a ring structure implies that there is superposition/interference in that representation.</li>
</ul>
<p>They find a bunch of features that activate on a <a href="https://transformer-circuits.pub/2025/linebreaks/index.html#char-count">range of given line widths</a>. Since two are usually active at a time, they imagine a curve connecting all these features. If they take PCA across 150 different line width averages for Layer 2, they find a 6-dimensional subspace that explains 95% of the variance across line width averages. They can also reconstruct the activations <em>only</em> using the SAE features above, and the curves track each other fairly closely.</p>
<ul>
<li>This is really interesting because there is no human-intuitive reason why line widths would have to be represented as a curve, right? Unless there was something about a 60-length line that nicely parallels a 30-length line? This really might be an artifact of superposition, because we don&rsquo;t want to use up an entire dimension, so we snake it around like so; probably because you&rsquo;re unlikely to confuse 60 and 30 anyway(?)</li>
<li>Q: do all of their averaged-line-width features come from tokens within documents with a max line width of 150, or do they come from different documents with different <em>max</em> line widths?</li>
<li>If we were trying to find this using some geometric DAS, why would we assume we&rsquo;re looking for a curved manifold?</li>
</ul>

        </div>
        

    



<div class="post-info">
    

    <a class="post-hidden-url u-url" href="http://localhost:1313/geometry/">http://localhost:1313/geometry/</a>
    <a href="http://localhost:1313/" class="p-name p-author post-hidden-author h-card" rel="me">Sheridan Feucht</a>


    <div class="post-taxonomies">
        
        
        
    </div>
</div>

    </article>

    
        
        
    

    

    

        </main>
        
            <footer class="common-footer">
    
    

    <div class="common-footer-bottom">
        
        <div class="copyright">
            <p>© Sheridan Feucht, 2026<br>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.<br>
            
            </p>  
        </div> 

        

    





<script>
const STORAGE_KEY = 'user-color-scheme'
const defaultTheme = "auto-without-switcher"

let currentTheme
let switchButton
let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

const autoChangeScheme = e => {
    currentTheme = e.matches ? 'dark' : 'light'
    document.documentElement.setAttribute('data-theme', currentTheme)
    changeButtonText()
}

document.addEventListener('DOMContentLoaded', function() {
    switchButton = document.querySelector('.theme-switcher')
    currentTheme = detectCurrentScheme()
    if (currentTheme == 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark')
    }
    if (currentTheme == 'auto') {
        autoChangeScheme(autoDefinedScheme);
        autoDefinedScheme.addListener(autoChangeScheme);
    }

    if (switchButton) {
        changeButtonText()
        switchButton.addEventListener('click', switchTheme, false)
    }
  
    showContent()
})

function detectCurrentScheme() {
    if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
        return localStorage.getItem(STORAGE_KEY)
    } 
    if (defaultTheme) {
        return defaultTheme
    } 
    if (!window.matchMedia) {
        return 'light'
    } 
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        return 'dark'
    }
    return 'light'
}

function changeButtonText()
{   
    if (switchButton) {
        switchButton.textContent = currentTheme == 'dark' ?  "❂" : "☾"
    }
}

function switchTheme(e) {
    if (currentTheme == 'dark') {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'light')
        document.documentElement.setAttribute('data-theme', 'light')
        currentTheme = 'light'
    } else {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'dark')
        document.documentElement.setAttribute('data-theme', 'dark')
        currentTheme = 'dark'
    }
    changeButtonText()
}

function showContent() {
    document.body.style.visibility = 'visible';
    document.body.style.opacity = 1;
}
</script>

   
    </div>

    <p class="h-card vcard">

    <a href=http://localhost:1313/ class="p-name u-url url fn" rel="me">Sheridan Feucht</a> 

     
        /
        <a class="p-email u-email email" rel="me" href="mailto:feucht.s@northeastern.edu">feucht.s@northeastern.edu</a>
    

     
        <img class="u-photo" src="/images/me.png" />
    
</p> 
</footer>

        
    </div>
</body>
</html>
