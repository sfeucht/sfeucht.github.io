<!DOCTYPE html>


<html lang="en-us" data-theme="">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Sheridan Feucht</title>

<meta name="description" content="Writing That is Still Not Meaningful?

    
        
    

December 2, 2025

What exactly are we doing when we read LLM-generated text?

Recently, a nonsense LLM-generated paper kicked up some outrage in the AI community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and later rejected for violating conference guidelines, I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer&rsquo;s comments:">





<link rel="icon" type="image/x-icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="http://localhost:1313/favicon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



    



<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>




    





    
    
    

    
        <link rel="stylesheet" href="http://localhost:1313/css/style.78de91526730265cde940d6082f8460bc8c2b62e4eddf9093c33d728050dcc87.css" integrity="sha256-eN6RUmcwJlzelA1ggvhGC8jCti5O3fkJPDPXKAUNzIc=">
    





    

    





    
    
    

    
        <script src="http://localhost:1313/js/script.32e751300d2d69e2cb56a98d2c9d964d54a53a8fb7281ccdbd80f055c88e8d86.js" type="text/javascript" charset="utf-8" integrity="sha256-MudRMA0taeLLVqmNLJ2WTVSlOo&#43;3KBzNvYDwVciOjYY="></script>
    







<meta property="og:url" content="http://localhost:1313/rerereading/">
  <meta property="og:site_name" content="Sheridan Feucht">
  <meta property="og:title" content="Sheridan Feucht">
  <meta property="og:description" content="Writing That is Still Not Meaningful? December 2, 2025
What exactly are we doing when we read LLM-generated text? Recently, a nonsense LLM-generated paper kicked up some outrage in the AI community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and later rejected for violating conference guidelines, I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer’s comments:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Sheridan Feucht">
  <meta name="twitter:description" content="Writing That is Still Not Meaningful? December 2, 2025
What exactly are we doing when we read LLM-generated text? Recently, a nonsense LLM-generated paper kicked up some outrage in the AI community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and later rejected for violating conference guidelines, I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer’s comments:">



    
        <link rel="webmention" href="https://webmention.io/hugo-theme-anubis/webmention" />
        
            <link rel="pingback" href="https://webmention.io/hugo-theme-anubis/xmlrpc" />
        
    
    
        <link rel="webmention" href="https://sfeucht.github.io/webmentions/receive" />
    







    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header"> 
            
                <div class="header-top">
    <h1 class="site-title">
    <a href="/">Sheridan Feucht</a>
</h1>
    <ul class="social-icons">


    
        <li>
            <a href="/research" title="Research" rel="me">
                <span class="social-text">
    research
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/blog" title="Blog" rel="me">
                <span class="social-text">
    blog
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://twitter.com/sheridan_feucht" title="Twitter" rel="me">
            <span class="social-text">
    twitter
</span>



            </a>
        </li>
    

    
        <li>
            <a href="https://bsky.app/profile/sfeucht.bsky.social" title="Bluesky" rel="me">
                <span class="social-text">
    bluesky
</span>



            </a>
        </li>
    

    
        
        
        <li>
            <a href="https://github.com/sfeucht" title="Github" rel="me">
            <span class="social-text">
    github
</span>



            </a>
        </li>
    

    
        <li>
            <a href="/sfeucht_cv.pdf" title="Cv" rel="me">
                <span class="social-text">
    cv
</span>



            </a>
        </li>
    




</ul>
</div>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



            
        </header>
        <main id="main" tabindex="-1"> 
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title"></h1>

                
            </header>
        </div>
        






        <div class="content e-content">
            <h1 id="writing-that-is-still-not-meaningful" >Writing That is Still Not Meaningful?
<span>
    <a href="#writing-that-is-still-not-meaningful">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h1><p>December 2, 2025</p>
<span style="color:gray">
<i>What exactly are we doing when we read LLM-generated text?</i>
</span>
<p>Recently, a <a href="https://x.com/micahgoldblum/status/1989088547777966512?s=20">nonsense LLM-generated paper</a> kicked up some outrage in the AI community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and <a href="https://x.com/iclr_conf/status/1989349884227715257?s=20">later rejected for violating conference guidelines</a>, I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer&rsquo;s comments:</p>
<p><img src="/review3_big.png" alt="I found this paper very difficult to read and comprehend. Beginning with the abstract—which conveys almost no meaningful insight to non-expert readers—the paper remains largely opaque throughout. It introduces numerous topics without proper context or explanation, ultimately leading to the unsubstantiated claim that a &ldquo;verification framework&rdquo; has been established.
There are two possible explanations for this lack of clarity: (1) The paper may be written in an extremely dense and narrow style, understandable only to experts working directly in this specific subarea (which I am not), or (2) The extensive use of LLM-assisted writing tools may have resulted in text that appears technically sophisticated but lacks genuine substance or coherence."></p>
<p>The frustration here resonated with me a lot—probably because I&rsquo;d been in a very similar situation before.</p>
<p>In 2020, I close-read approximately 118 AI-generated documents on cannabis legalization (and an equal number of human-written ones). It was a painstaking task: I needed to classify the <a href="https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23722/1/scidok_final.pdf">lexical aspectual class</a> of every clause, mark coherence relations between clauses, and rate the argumentation quality of each document. But there were two things that made this difficult. First, I didn&rsquo;t know which articles were human-written and which were AI-generated. Second, the AI-generated documents were extremely uncanny. Take this sentence, for example:</p>
<blockquote>
<p><tt>If weed’s not really a public health issue and you&rsquo;re really happy about it, get an understanding about the ways in which it will be able to influence your behaviour.</tt></p>
</blockquote>
<p>Is this someone&rsquo;s Reddit comment, posted without a second thought? Or is it a semi-competent language model&rsquo;s attempt to imitate the surface form of an argument? I was never really sure. But the quality of my annotations depended on actually understanding what was being said here, so I spent a lot of time re-reading these kinds of sentences over and over, trying to grasp some kind of meaning from them. It felt like I was having a stroke, or like I was being gaslit by the text. But then, I&rsquo;d read something like</p>
<blockquote>
<p><tt>BART police already have a &ldquo;marijuana alley&rdquo; where potential customers could find sprayers and pagers ready to use and find it where they&rsquo;re supposed to.</tt></p>
</blockquote>
<p>and breathe a sigh of relief. I&rsquo;d know that this document is nonsense—that it was generated by GPT-2, a disembodied probabilistic model that cannot smoke weed and has never been to the Bay Area. Thus, I felt safe in assuming that there was &ldquo;nothing there&rdquo; for me to annotate.</p>
<p>The following summer (2021), I wrote an <a href="https://sfeucht.github.io/rereading">essay</a> about this experience for a class. It includes some cool examples of AI-generated nonsense that has the <em>shape</em> of a sensible argument, without the content. In that essay, I argued that the text generated by LLMs is odd in that it is not language <em>yet</em>—not until a human is able to read and extract meaning from that text. In this way, LLM-generated text is strangely beautiful.</p>
<h2 id="writing-thats-not-a-paper" >Writing That&rsquo;s Not a Paper
<span>
    <a href="#writing-thats-not-a-paper">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>LLMs are a lot better now than they were in 2020: almost every sentence that comes out of a frontier model is not only structurally coherent, but sensical in the context of the sentences that came before it. You can almost always follow the flow of an LLM-generated article from beginning to end without much difficulty. So, does that mean that these documents are just&hellip; meaningful now?</p>
<p>Maybe not. There&rsquo;s no fundamental difference between what GPT-2 does and what GPT-5 does (to our knowledge). If GPT-2 gave us <em>sentences</em> that looked correct but did not actually say anything, then maybe GPT-5 gives us entire <em>papers</em> that look coherent but don&rsquo;t actually make any sense. Here&rsquo;s a screenshot I took of the culprit paper&rsquo;s introduction as an example.</p>
<p><img src="/intro.png" alt="Deploying large language models in sensitive settings creates a need for verifiable inference: proofs that outputs were computed correctly without revealing proprietary weights or private inputs. Zero-knowledge ML (ZKML) offers this, yet current systems struggle to scale to Transformer architectures.
Existing frameworks compile networks into polynomial constraint systems; prover cost is dominated by the number of constraints (roughly linear in parameter count). Cryptographic advances (e.g., lookups, sumcheck, commitments) accelerate the protocol layer, but they do not remove the model-level redundancy intrinsic to attention—leaving many constraints structurally unnecessary.
Key idea (GaugeZKP). Exploit attention&rsquo;s gauge symmetries. Many parameterizations implement the same function. We rewrite deployed weights into a canonical form (constructed per head; see §3.4) without changing the model. A one-time Proof of Gauge Equivalence (PoGE) binds deployed and canonical weights; thereafter, per-inference Proofs of Verifiable Inference (PoVI) run only on the canonical model. Because this optimization is upstream of the prover, it composes with protocol-level speedups.
Scope and guarantees. We certify exact functional equivalence (no approximation); privacy follows from the zk proof system. Attention remains quadratic in sequence length. Canonicalization relies on full-column-rank projections and numerically stable QR/SPD roots; fixed-point precision uses scale 2^16 (deterministic choices yield identical outputs across implementations)."></p>
<p>I know nothing about this area, so I&rsquo;ll leave critique of the &ldquo;substance&rdquo; of this paper to others (<em>if</em> that substance exists; more on that later). But what struck me was that this introduction has all of the surface-level indicators of being cogent and well-written: plain language, italicized key terms, and even a bolded paragraph header that points you to the <strong>Key Idea.</strong> Nonetheless, when I read it, I have no idea what problem the &ldquo;authors&rdquo; were trying to solve, or how their &ldquo;key idea&rdquo; actually helps to solve it.</p>
<p>The way I feel reading this introduction is how I used to feel reading technical papers in undergrad, when I first started trying to get into ML research. Scanning over the text, it looks like something that should make a lot of sense to an expert somewhere, but no matter how many times I re-read any sentence, I am no closer to understanding what&rsquo;s going on. It could be because I don&rsquo;t know about ZKML, but I don&rsquo;t think this is true. If I read the introduction of <a href="https://arxiv.org/pdf/2404.16109">this real paper</a> on zero-knowledge proofs for LLMs, it&rsquo;s like a breath of fresh air: I actually understand what ZKML is, why it&rsquo;s interesting and hard, and even get a rough sense of what the authors did (even though it would take lots of work for me to truly understand it).</p>
<p>Unfortunately, the folks who had to review this paper were subjected to worse LLM-gaslighting than I ever had to experience in my annotator days. In 2020, when I re-re-read a clause like &ldquo;potential customers could find sprayers and pagers ready to use and find it where they&rsquo;re supposed to,&rdquo; the escape hatch was always right there (I knew it could be AI). But here, not only was the nonsensity of the text much more subtle, but the possibility that this paper was AI-generated might not have even crossed the reviewers&rsquo; minds. If I was in this situation, I might have felt that old insecurity from undergrad creeping in, a nagging feeling that the problem is <em>me</em>, maybe even that I should keep my head down and not object. I can see that being a factor for why this could get past so many reviewers.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<h2 id="what-even-is-reading" >What Even Is Reading?
<span>
    <a href="#what-even-is-reading">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>This specific frustration of re-re-reading sentences to no avail isn&rsquo;t new; I would guess that most people have experienced it with human-written text. I know that I can sometimes get &ldquo;stuck&rdquo; on sentences, reading them over and over like a broken record. For me, this mostly happens when reading technical writing, but I&rsquo;ve also experienced it reading fiction, and even forum posts.</p>
<p>Reading is usually very easy for us, and it almost happens involuntarily. Think of the Stroop effect, where it&rsquo;s hard to <em>not</em> read the content of a word when it&rsquo;s flashed in front of you. When we read text that&rsquo;s written by others, we understand their thoughts and intentions quite quickly, almost as if there&rsquo;s a wire conducting their throughts straight into our brains. This might be why, at least in western English-speaking cultures, we conceptualize language as a conduit for meaning. This is known as the <span style="font-variant:small-caps;">Conduit Metaphor</span>, <a href="http://www.biolinguagem.com/ling_cog_cult/reddy_1979_conduit_metaphor.pdf">first described</a> by linguist Michael J. Reddy in 1979, who argued that it forms the basis of how English speakers conceptualize communication and meaning.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>Under the <span style="font-variant:small-caps;">Conduit Metaphor</span>, we think of thoughts/ideas as objects in our minds that can be &ldquo;transferred&rdquo; to other people. There&rsquo;s two ideas going on here: first, that communication is the process of transferring thoughts to other people, and second, that language is the <em>container</em> in which we put those thoughts (&ldquo;put those ideas in some other paragraph,&rdquo; &ldquo;her words were filled with emotion&rdquo;). If you study language, you&rsquo;ve probably come across this assumption stated explicitly; for example, information-theoretic modeling of language conceptualizes communication as a noisy channel, where we encode meaning into messages that are then sent along this channel.</p>
<p>However, as Reddy points out in his original essay, this metaphor is misleading. We can never actually access or experience the mind of another person, and ideas are not objects that we can take from our minds and wire directly into other people&rsquo;s brains. If we could actually do that, we wouldn&rsquo;t need language at all.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> Instead, communication is more like sending someone a recipe for a specific dish. Recipes do not inherently &ldquo;contain&rdquo; any food in them. Every person making a particular recipe will interpret the instructions differently, depending on their kitchen and the ingredients that they have access to in their own home, sometimes leading to very different outcomes. It would be nice if we could send our friends food (i.e., thoughts) directly, but because we don&rsquo;t have a way to establish direct brain-to-brain contact, we have to make do with recipes (i.e., language).</p>
<!-- Instead of thinking of reading in terms of the <span style="font-variant:small-caps;">Conduit Metaphor</span>, where a given text "contains" meaning that must be unpackaged by the reader, we could think of reading as a process of *generating* meaning from some object that exists in the world.  -->
<!-- [recipes do not inherently contain food in them.] Under this conceptualization, the process of reading is kind of like making a recipe from a cookbook—every person making a recipe will interpret the instructions differently, depending on their kitchen and the ingredients that they have access to in their own home, leading to very different outcomes. It would be nice if we could "order takeout" and directly transmit ideas into other people's brains, but since that is impossible, we have to make do with recipes/language.   -->
<!-- In this analogy where ideas are food, the <span style="font-variant:small-caps;">Conduit Metaphor</span> would be akin to ordering UberEats (transporting someone else's food directly into your home). But since we can't actually directly transport ideas from one head to another, we have to make do with recipes, which we all interpret differently based on our own dietary restrictions and physical ingredients on hand.  -->
<!-- This captures the idea that when we read a text, we don't actually have access to the meaning [^3]  -->
<!--  -->
<p>Under this &ldquo;recipe&rdquo; metaphor,<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> meaning is not inherently contained within a text, but <em>comes about</em> as a result of a reader interpreting that text. I like this idea because it seems to resonate with other ways in which we use the word &ldquo;read.&rdquo; When we read tea leaves, we are generating meaning from a random blob based on our own personal symbols and conceptual structures. You can <em>read</em> someone&rsquo;s face, or &ldquo;read into&rdquo; their actions, but those interpretations will always be in terms of your own disposition, your own hopes and anxieties. The act of reading is always interpretation, rather than extraction: when someone says, &ldquo;that&rsquo;s my reading of it,&rdquo; they are being totally precise, because every person will read a given text slightly differently.</p>
<p>So what is happening when we find ourselves stuck re-re-reading the same sentence? If we accept that there is never any meaning &ldquo;contained&rdquo; inside a text, then these moments are not <em>failures</em> to extract some true meaning lurking inside a work, but simply moments where some symbols on a page are not triggering any ideas in our minds. This could happen for any number of reasons: fatigue, failure on the author&rsquo;s part to phrase their thoughts clearly, or simply a lack of relevant conceptual structure on the part of the reader (like when you try to read technical writing on an unfamiliar topic). And of course, it could happen when trying to read an artefact generated by a language model imitating the <em>form</em> of a well-written argument, without any of the content. These moments show us that reading has always been a process of <em>generating</em> meaning, not extracting it.</p>
<h2 id="ants-meaningfully-fornicating-in-the-sand" >Ants Meaningfully Fornicating In the Sand
<span>
    <a href="#ants-meaningfully-fornicating-in-the-sand">
        <svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
    </a>
</span>
</h2><p>Under this framing, if a piece of text is meaningful to a reader, it doesn&rsquo;t really matter <em>how</em> that text came into being: whether it was human-written, AI-generated, or <a href="https://arxiv.org/pdf/2308.05576">formed by some ants in the dirt</a>. This is starting to get into philosophical territory; since I don&rsquo;t have a background in philosophy, I&rsquo;ll just plainly state the issue that arises for me here.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>This &ldquo;ants in the dirt&rdquo; example that I linked to is from Hilary Putnam&rsquo;s 1981 book, <em>Reason, Truth, and History.</em> I learned about it from <a href="https://arxiv.org/pdf/2308.05576">Matthew Mandelkern and Tal Linzen&rsquo;s 2024 article</a>, &ldquo;Do Language Models&rsquo; Words Refer?&rdquo;, who explain it very well:</p>
<blockquote>
<p>Suppose that, at a picnic, you observe ants wending through the sand in a surprising pattern, which closely resembles the English sentence &ldquo;Peano proved that arithmetic is incomplete&rdquo;. At the same time, you get a text message from Luke, who is taking a logic class. He writes, &ldquo;Peano proved that arithmetic is incomplete&rdquo;.</p>
<p>Intuitively, the two cases are very different, despite involving physically similar
patterns. The ants’ patterns do not say anything; they just happen to have formed
patterns which resemble meaningful words. Of course, you can interpret the pattern,
just as you can interpret an eagle’s flight as an auspicious augur; but these are interpretations you overlay on a natural pattern, not meanings intrinsic to the patterns themselves. By contrast, Luke’s words mean something definite on their own (regardless of whether you or anyone else interprets them): namely, that Peano proved that arithmetic is incomplete. What Luke said is false: it was Gödel who proved incompleteness. But
Luke said something, whereas the ants didn’t say anything at all.</p>
</blockquote>
<!-- In particular, he said something false about Peano, which means that his (use of the) word ‘Peano’ managed
to refer to Peano. -->
<p>If we apply the &ldquo;recipe metaphor&rdquo; to this scenario, then the meaning that arises in our minds when we read &ldquo;Peano proved that arithmetic is incomplete&rdquo; is the same for Luke&rsquo;s text and for the ants in the dirt. In both scenarios, &ldquo;Peano proved that arithmetic is incomplete&rdquo; induces the exact same thought in our head (apart from non-linguistic concerns, like &ldquo;why is Luke texting me this?&rdquo; or &ldquo;how the hell did these ants happen to spell out an entire sentence?&rdquo;). But crucially, this meaning is <em>not</em> &ldquo;intrinsic to the patterns themselves&rdquo;—it arises only when we see and interpret those patterns. And if someone else were to see the exact same patterns, the meaning that they would derive in their heads would be slightly different.</p>
<p>This is what confuses me about debates on whether LLM-generated text is &ldquo;truly meaningful.&rdquo; If I read a sentence that causes me to have a particular thought, then I have made meaning from that sentence, and so the sentence is meaningful (to me). It used to be that LLM-generated text was hard to interpret, which meant that most of the time, it was not meaningful—unless you were a poor annotator like me, whose job it was to try very hard to interpret horrible things like <a href="https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html">&ldquo;It’s all just the right amount of subtlety in male porn, and the amount of subtlety you can detect is simply astounding.&quot;</a> But now, LLM-generated text is so good that the majority of it is meaningful to most people almost all of the time.</p>
<p>[the difference between luke and the ants maybe has to do with all of those metalinguistic considerations that go on when you read their sentences. knowing the context behind someone, and guessing their intentions, really helps with interpretation.] The question of communicative intent is different, though (and again, is where my lack of background is probably showing). [talk about what it means to write under this framework, trying to induce an idea in someone else&hellip;]</p>
<p>There are some infamous examples of fake papers making it into philosophy journals&hellip;</p>
<ul>
<li>even well-written human text. it&rsquo;s not that we convert thoughts to words and words back to thoughts. it&rsquo;s not a symmetric process. you can have thoughts you don&rsquo;t write down, but more importantly, people can read anything out of your words that you weren&rsquo;t thinking of. is this a continuum?</li>
</ul>
<p>But&hellip; what <em>if</em> the thing that the reviewers read from the work was actually good?</p>
<ul>
<li>fake paper examples that david mentioned. yeah like who cares if the author thought it was fake, if they did actually say something interesting</li>
<li>derrida example?</li>
</ul>
<p>In a post-LLM world, we are now flooded with &ldquo;meaningless&rdquo; slop TODO talk about &ldquo;faith&rdquo; and generating meanings&hellip;</p>
<!-- banging your head against a really difficult to understand paper. you can only do this if you have faith that the writer was a human who had something important they wanted to say.  -->
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Of course, the right thing to do in this situation would always be to ask for the paper to be reassigned, or review to the best of your ability with low confidence; if a paper is that hard to understand, it probably shouldn&rsquo;t get published anyway. But a willingness to call BS seems to be a combination of seniority and personality.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I learned about this idea from George Lakoff and Mark Johnson&rsquo;s book, <a href="https://en.wikipedia.org/wiki/Metaphors_We_Live_By"><em>Metaphors We Live By</em></a>, published in 1980. Thank you to Si Wu for recommending this book to me!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Maybe the world would look something like the Human Instrumentality Project from Evangelion.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Reddy&rsquo;s name for this is actually the &ldquo;toolmakers paradigm,&rdquo; but since the way I think about it might not match his original essay exactly, I won&rsquo;t use his term in this post.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>If you have a philosophy background and are interested in talking about this, please reach out! This has been bothering me a lot.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

        </div>
        

    



<div class="post-info">
    

    <a class="post-hidden-url u-url" href="http://localhost:1313/rerereading/">http://localhost:1313/rerereading/</a>
    <a href="http://localhost:1313/" class="p-name p-author post-hidden-author h-card" rel="me">Sheridan Feucht</a>


    <div class="post-taxonomies">
        
        
        
    </div>
</div>

    </article>

    
        
        
    

    

    

        </main>
        
            <footer class="common-footer">
    
    

    <div class="common-footer-bottom">
        
        <div class="copyright">
            <p>© Sheridan Feucht, 2025<br>
            Powered by <a target="_blank" rel="noopener noreferrer" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" rel="noopener noreferrer" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.<br>
            
            </p>  
        </div> 

        

    





<script>
const STORAGE_KEY = 'user-color-scheme'
const defaultTheme = "auto-without-switcher"

let currentTheme
let switchButton
let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

const autoChangeScheme = e => {
    currentTheme = e.matches ? 'dark' : 'light'
    document.documentElement.setAttribute('data-theme', currentTheme)
    changeButtonText()
}

document.addEventListener('DOMContentLoaded', function() {
    switchButton = document.querySelector('.theme-switcher')
    currentTheme = detectCurrentScheme()
    if (currentTheme == 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark')
    }
    if (currentTheme == 'auto') {
        autoChangeScheme(autoDefinedScheme);
        autoDefinedScheme.addListener(autoChangeScheme);
    }

    if (switchButton) {
        changeButtonText()
        switchButton.addEventListener('click', switchTheme, false)
    }
  
    showContent()
})

function detectCurrentScheme() {
    if (localStorage !== null && localStorage.getItem(STORAGE_KEY)) {
        return localStorage.getItem(STORAGE_KEY)
    } 
    if (defaultTheme) {
        return defaultTheme
    } 
    if (!window.matchMedia) {
        return 'light'
    } 
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        return 'dark'
    }
    return 'light'
}

function changeButtonText()
{   
    if (switchButton) {
        switchButton.textContent = currentTheme == 'dark' ?  "❂" : "☾"
    }
}

function switchTheme(e) {
    if (currentTheme == 'dark') {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'light')
        document.documentElement.setAttribute('data-theme', 'light')
        currentTheme = 'light'
    } else {
        if (localStorage !== null)
            localStorage.setItem(STORAGE_KEY, 'dark')
        document.documentElement.setAttribute('data-theme', 'dark')
        currentTheme = 'dark'
    }
    changeButtonText()
}

function showContent() {
    document.body.style.visibility = 'visible';
    document.body.style.opacity = 1;
}
</script>

   
    </div>

    <p class="h-card vcard">

    <a href=http://localhost:1313/ class="p-name u-url url fn" rel="me">Sheridan Feucht</a> 

     
        /
        <a class="p-email u-email email" rel="me" href="mailto:feucht.s@northeastern.edu">feucht.s@northeastern.edu</a>
    

     
        <img class="u-photo" src="/images/me.png" />
    
</p> 
</footer>

        
    </div>
</body>
</html>
