<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sheridan Feucht</title>
    <link>http://localhost:1313/</link>
    <description>Sheridan Feucht</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Feb 2019 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;p&gt;Written in Go, Hugo is an open source static site generator available under the &lt;a href=&#34;https://github.com/gohugoio/hugo/blob/master/LICENSE&#34;&gt;Apache License 2.0.&lt;/a&gt; Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.&lt;/p&gt;
&lt;p&gt;Hugo makes use of a variety of open source projects including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yuin/goldmark&#34;&gt;https://github.com/yuin/goldmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/alecthomas/chroma&#34;&gt;https://github.com/alecthomas/chroma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/muesli/smartcrop&#34;&gt;https://github.com/muesli/smartcrop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spf13/cobra&#34;&gt;https://github.com/spf13/cobra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spf13/viper&#34;&gt;https://github.com/spf13/viper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.&lt;/p&gt;
&lt;p&gt;Hugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.&lt;/p&gt;
&lt;p&gt;Websites built with Hugo are extremely fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.&lt;/p&gt;
&lt;p&gt;Learn more and contribute on &lt;a href=&#34;https://github.com/gohugoio&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/</guid>
      <description>&lt;h1 id=&#34;sheridans-blog&#34; &gt;Sheridan&amp;rsquo;s Blog
&lt;span&gt;
    &lt;a href=&#34;#sheridans-blog&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;!-- [Writing That is Language Now?](/rerereading) &lt;br&gt;
November 15, 2025 • &lt;span style=&#34;color:gray&#34;&gt;&lt;i&gt;A follow-up to &#34;Writing That&#39;s Not Language (Yet).&#34;&lt;/i&gt;&lt;/span&gt; --&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/syllogisms&#34;&gt;Solving Syllogisms is Not Intelligence&lt;/a&gt; &lt;br&gt;
April 24, 2025 • &lt;span style=&#34;color:gray&#34;&gt;&lt;i&gt;I think that we overvalue logical reasoning when it comes to measuring &amp;ldquo;intelligence.&amp;quot;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sfeucht.github.io/rereading&#34;&gt;Writing That&amp;rsquo;s Not Language (Yet)&lt;/a&gt; &lt;br&gt;
August 10, 2021 • &lt;span style=&#34;color:gray&#34;&gt;&lt;i&gt;An interactive essay I wrote for an undergraduate seminar on the theory and practice of writing.&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/books2024/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/books2024/</guid>
      <description>&lt;h1 id=&#34;books-i-read-in-2024&#34; &gt;Books I Read in 2024
&lt;span&gt;
    &lt;a href=&#34;#books-i-read-in-2024&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;p&gt;Here are the books that I remember reading this year. I don&amp;rsquo;t set reading goals for myself, but this year a lot of interesting books fell into my lap. I even managed to have time to read many of them!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;East, West - Salman Rushdie&lt;/li&gt;
&lt;li&gt;If This Is a Man - Primo Levi&lt;/li&gt;
&lt;li&gt;How to Do Things with Words - JL Austin&lt;/li&gt;
&lt;li&gt;Lolita - Vladimir Nabokov&lt;/li&gt;
&lt;li&gt;Seeing Further - Esther Kinsky&lt;/li&gt;
&lt;li&gt;The Empusium - Olga Tokarczuk&lt;/li&gt;
&lt;li&gt;Childish Literature - Alejandro Zambra&lt;/li&gt;
&lt;li&gt;The Ways of Paradise - Peter Cornell&lt;/li&gt;
&lt;li&gt;Attached - Amir Levine and Rachel S. F. Heller&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### East, West - Salman Rushdie.

My partner Adrian bought this from a bin in Harvard Square. I found it delightful, and will be looking for more Salman Rushdie in the future. 

**If This Is a Man - Primo Levi.** Lent to me by my friend Raj. 

**How to do things with words - JL Austin.** Lent to me by my advisor David. Makes me want to read Judith Butler. 

**Lolita - Nabokov.** One of the best novels I&#39;ve read so far, ever. 

**Seeing Further - Esther Kinsky.** Gift from my partner (first book from a Fitzcarraldo Editions subscription). 

**The Empusium - Olga Tokarczuk.** 

**Childish Literature - Alejandro Zambra.**

**The Ways of Paradise - Peter Cornell.**

**Attached - Amir Levine and Rachel S. F. Heller.**  --&gt;
&lt;h2 id=&#34;unfinished&#34; &gt;Unfinished
&lt;span&gt;
    &lt;a href=&#34;#unfinished&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;&lt;p&gt;Here are some books I enjoyed this year but didn&amp;rsquo;t finish. I&amp;rsquo;m hoping to pick at least a few of them back up next year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Art of Dramatic Writing - Lajos Egri&lt;/li&gt;
&lt;li&gt;3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool - James Kaplan&lt;/li&gt;
&lt;li&gt;The Enigma of Reason - Dan Sperber and Hugo Mercier&lt;/li&gt;
&lt;li&gt;Psychology and the East - Carl Jung&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/books2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/books2025/</guid>
      <description>&lt;h1 id=&#34;books-i-read-in-2025&#34; &gt;Books I Read in 2025
&lt;span&gt;
    &lt;a href=&#34;#books-i-read-in-2025&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;(Jan) Love in the Time of Cholera - Gabriel García Márquez&lt;/li&gt;
&lt;li&gt;(Jan) Morning and Evening - Jon Fosse&lt;/li&gt;
&lt;li&gt;(March) Pale Fire - Vladimir Nabokov&lt;/li&gt;
&lt;li&gt;(April) Orality and Literacy - Walter Ong&lt;/li&gt;
&lt;li&gt;(May) Syntactic Structures - Noam Chomsky&lt;/li&gt;
&lt;li&gt;(June) How to Write About Contemporary Art - Gilda Williams&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/headphones/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/headphones/</guid>
      <description>&lt;h1&gt;Dan&#39;s Headphones&lt;/h1&gt;
&lt;p&gt;A meandering list of tunes and tracks that I enjoy, updated ~monthly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[11/2023] &lt;a href=&#34;https://www.youtube.com/watch?v=G2RFKpPZcow&#34;&gt;Hallucinations - Keith Jarrett Trio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[02/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=svoGEnDX95c&#34;&gt;Invitation - Joe Henderson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[03/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=K63CD2pwjD0&#34;&gt;Wednesday Morning, 3 A.M. - Simon &amp;amp; Garfunkel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[04/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=8NjUxjsnKgo&#34;&gt;Up Jumped Spring - Christian McBride Big Band&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[05/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=EwS0ccjya_I&#34;&gt;Stretto From The Ghetto - Branford Marsalis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[06/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=0E5l2GHBxB8&#34;&gt;Stal - C418&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[07/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=Hvz0TOm0zgI&#34;&gt;Only A Fool Would Say That - Steely Dan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[07/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=tnV7dTXlXxs&#34;&gt;Ventura Highway - America&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[08/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=t4ywIPrewpg&#34;&gt;Out on the Weekend - Neil Young&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[09/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=7ci7oJIkP2Q&#34;&gt;Tangerine - Christian McBride Live Session&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[10/2024] &lt;a href=&#34;https://www.youtube.com/watch?v=ABQjT6gDKu0&#34;&gt;Dissolved Girl - Massive Attack&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.youtube.com/watch?v=fS7XPtFTvb8&#34;&gt;Beginners Falafel - Flying Lotus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[01/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=WMDWPH4oKwo&#34;&gt;Type Slowly - Pavement&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.youtube.com/watch?v=K14qg9E9SoE&#34;&gt;Slowly Typed - Pavement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[02/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=9KE_I6d5m9E&#34;&gt;Armando&amp;rsquo;s Rhumba - Chick Corea&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[04/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=wiLIV3H0q-Y&#34;&gt;Can&amp;rsquo;t We Be Friends - Ella &amp;amp; Louis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[06/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=Claf8E18eLs&#34;&gt;You&amp;rsquo;re Gonna Make Me Lonesome When You Go - Bob Dylan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[07/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=q4KYYRzFUzE&#34;&gt;Vampire in the Corner - Magdalena Bay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[09/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=1XtUK5Boy7o&#34;&gt;Sunset - McCoy Tyner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[11/2025] &lt;a href=&#34;https://www.youtube.com/watch?v=TaKD1Vdarnw&#34;&gt;King Harvest (Has Surely Come) - The Band&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/manny/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/manny/</guid>
      <description>&lt;h1 id=&#34;man-my-first-and-last-friend&#34; &gt;Man, My First and Last Friend
&lt;span&gt;
    &lt;a href=&#34;#man-my-first-and-last-friend&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;p&gt;&lt;span style=&#34;color:gray&#34;&gt; May 3, 2025 - A short story about high schoolers. Congrats if you found this page. &lt;a href=&#34;http://localhost:1313/manny.pdf&#34;&gt;PDF Version&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Man (Manny) is a kid in my grade. How do I explain him to you? Well, first, he was born in 2001. This means, like me, he is sixteen years old. He mains Fox. His outward temperament is soft, but his insides are littered with naval mines. He is the only person I have ever loved. Oh, and just like me, he loves &lt;code&gt;MEME&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Yes, &lt;code&gt;MEME&lt;/code&gt;—that is a good place to start. Two years ago after Labour Day, I sat beneath a window of cold autumn light. I was fourteen at the time. A muffin wrapper filled with jelly beans lay on the laminate table. The jellies reminded me of &lt;code&gt;THE BEAN MEME&lt;/code&gt;, and I needed this fact to be known.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“They’re just putting beans anywhere now, huh?” I quipped. I heard a snort behind me and turned to see the corners of Man’s mouth lift.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“I saw that &lt;code&gt;MEME&lt;/code&gt;,” he said with a hint of pride. His eyes were still fixed on the cup of candy.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“It’s a &lt;code&gt;DANK&lt;/code&gt; one,” I said. He looked up, and I noticed for the first time that his irises were dark green. “I don’t think I caught your name?” &lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“Sorry, yeah, I’m Manny. Or Man, whichever is easier.” &lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“Okay. I’m Zain!” I wanted to say, &lt;em&gt;Man is easier, because that’s what you are, a huMan,&lt;/em&gt; but thankfully I was a moment too late, and the quip was lost to time.&lt;/p&gt;
&lt;p&gt;Syllabi were passed around the room, and that was the extent of my interactions with Man on the day after Labour Day. This was mainly because neither of us knew how to ask someone else about themselves. As I walked home that afternoon, the sky was clear and cold. It had been a number of hours since I’d eaten the beans; I wondered where in my body the glucose and dye had ended up.&lt;/p&gt;
&lt;p&gt;I won’t say that I have no friends, because I do have friends by most metrics. Lucas, who was my lockermate in Grade 6. Amos, a Marth main who hates math. But I don’t think that these are close friends. When Amos shaved his head for charity, I watched from the atrium windows. I’ve never been to Lucas’ house. My mom has never met either set of parents. But I never say that I have no friends, because that is graceless. I eat lunch with them most days. What else do you call the people you lunch with?&lt;/p&gt;
&lt;p&gt;That said, Manny was my first real friend. While it took years to know Amos and Lucas, it took weeks to know him. After a few lonely attempts at gathering like terms, I would turn around to show Manny a watermarked &lt;code&gt;MEME&lt;/code&gt;, at which he would chuckle to signal his understanding of its depth; a warm reprieve from polynomial limbo for us both. My eyes would slide away from the phone screen, just in time to see Man’s flit down towards his papers. I had never met someone else who knew about &lt;code&gt;THE BEAN MEME&lt;/code&gt;. That was the difference between him and Marina, my previous deskmate and Grade 8 crush. &lt;em&gt;Marina never understood dank,&lt;/em&gt; I would reflect quietly. &lt;em&gt;Not like Manny does. Girls typically do not grasp the subtle qualities of&lt;/em&gt; &lt;code&gt;EDGY HUMOUR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Yes, I could be &lt;code&gt;EDGY&lt;/code&gt; with Manny. We were two teenage boys, after all! Once, I said the word “kinky,” and it made him laugh to tears. Together, we enjoyed fake salutes and ironic hot dogs. And every time I made him laugh, I felt blood in my cheeks.&lt;/p&gt;
&lt;p&gt;Now, it’s lunchtime in mid-October. I reach over outstretched legs to get to my locker. It’s embarrassing to eat in the hallways, but I don’t know where else to go, so I sit and look at Amos’ laptop, over his shoulder. But today was a special day. Manny fluttered in and kneeled across from me, back to the cinderblock, with his black Adidas pants folded up like the wings of a magpie. He was uncharacteristically forward, but his voice was as soft as always.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“Do you like calligraphy?” &lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;I don’t know what to say to this! I’ve never thought about calligraphy in my life. It’s old-person and gay at the same time. But&amp;hellip; “Yes?”&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;His corvid eyes sparkled. “You should come to the art room today, they’re doing a calligraphy thing!”&lt;/p&gt;
&lt;p&gt;I don’t exactly know what that means, but I stuff my Tupperware back into my bag and zip it up as we walk downstairs together&amp;ndash;just the two of us, since Amos and Lucas were not interested. Manny walks with a lilt, almost like he’s skipping down the stars. My throat felt slightly dry.&lt;/p&gt;
&lt;p&gt;The calligraphy itself was fine. You were supposed to be gentle but firm with the pen. I must have pressed too hard: I kept puncturing the white paper with my nib, scraping fibres away from the pink construction paper underneath. Manny carefully filled his paper with identical lowercase “a”s. For the first time, I sat next to him, instead of in front of him. As he carefully carved calligraphy “a”s, I reflected on his other lettering&amp;hellip; the way that he wrought algebraic variables, curvy-like. The sharp, blue-blue pencil that he used to point out my mistakes, as if he was carefully holding my hand as we crossed a creek. Yeah, he was smarter than me, and harder-working, but he was so soft and shy. Why did he keep practicing the letter “a”?&lt;/p&gt;
&lt;p&gt;Glancing over the other white sheafs, I noticed that his friends (girls he met through Drama who I didn’t really know) had already started writing real messages. None of them talked to me. They actually didn’t look at me at all, even when I cleared my throat. It could have been that they just loved lettering that much. Or, they were wondering what an &lt;code&gt;EDGY&lt;/code&gt; guy like me was doing in the mostly-female art room at lunch. I wasn’t autistic enough to break out the &lt;code&gt;MEMES&lt;/code&gt; in front of them, but I did spend the last ten minutes of lunch writing a number of uppercase “F”s (a subtle nod to the &lt;code&gt;PAY RESPECTS MEME&lt;/code&gt;). Manny didn’t seem to catch my reference, so we just parted with a “that was cool, see you on Thursday.”&lt;/p&gt;
&lt;p&gt;Here’s an analogy from simple machines: if all the events in my sixteen years of life were placed chronologically along a long piece of plywood and I needed to pick a location for a fulcrum, its plastic head would point to the moment when I sat down next to Manny at the calligraphy workshop. That was the first time I &lt;em&gt;smelled&lt;/em&gt; him. Green apples and clean dark hair. From that moment on, I was cursed to smell Manny’s Head and Shoulders everywhere: on the third floor, on the ground floor, in stacks of dusty textbooks, above the slush between the two sets of main doors, in my jacket and my papers, when he was at school, and especially when he was not. It was ambrosia. As an adolescent, there was of course a sexual aspect to my obsession. Manny was small, like a wren, with dark brows and ceramic hands. But he also laughed at my MEMEs, and we would play Smash together, and he would &lt;em&gt;come up&lt;/em&gt; to me at lunch. He would come up to me. He wanted to talk to me! I thought about him at night, but I also wanted to cup him in my hands. I wanted to stroke the top of his head with the backs of my fingers.&lt;/p&gt;
&lt;p&gt;This fixation spanned semesters. Weeks upon months of online quizzes and unsatisfactory porn. I figured that enough rolls of the dice (of asking “does he like me?”) would reveal his internals. Upon later reflection, my faith was not in the veracity of any one quiz, but rather in the statistical Law of Large Numbers. My sample mean was 52% YES. Throughout this process, I learned many digested facts about body language. Although Manny never touched me, he did laugh at all of my jokes, and sometimes his gaze drifted towards my mouth as I spoke. Agonizingly, this usually means that &amp;ldquo;they might like you!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But here is how I knew that he liked me for sure. On Sports Day in April, we had an extended lunch. We were in the same faction, the only two unpopular kids in our randomly-assigned group. We walked together to the gas station Tim Hortons: I got a chicken wrap, and Man got an Iced Capp. He practically fucking said it to me right then.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“I’m so tired from today,” he began.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“Same, I mean I don’t really ‘work out’ so like-“&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“Oh yeah, well, I kind of meant having to talk to all those random people?” Our eyes met in a flash of connection. He continued.&lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;“It’s just so hard to ‘small talk’, but it’s worse when you don’t talk at all, but everything I say&amp;hellip;” All I did was nod. To be crystal clear, there was no prompting here. &lt;br&gt;
&lt;span class=&#34;indent&#34;&gt;But then he trailed off, looked into my open eyes, and said, “But I feel so comfortable with you.”&lt;/p&gt;
&lt;p&gt;Fuck, man! It destroys me every time. I loved him, and it was so clear that he loved me too. He had to have.&lt;/p&gt;
&lt;p&gt;Now, to the worst part. When we were in Grade 10, we sat on the edge of the classroom for Math 20-1, close to the northern door. (Manny might mention that the dash one indicates the rigor of this challenging course.) Hannah was gone today, and I managed to sit with Man, alone. He seemed tired; he was darker than usual. But today was a special day, and I had to proceed with the operation. I opened the New Album on my phone, pretended to laugh at a &lt;code&gt;MEME&lt;/code&gt;, and thrusted my phone out expectantly (you know the drill, Manny). He looked up and squinted, but his face remained slack.&lt;br&gt; &lt;span class=&#34;indent&#34;&gt;“Oh, haha.” He looked back down at his lined paper, but didn’t write anything. &lt;br&gt;
What? I only had one more buffer before the big drop, and it evoked an equally tepid response. I hurriedly swiped past it and forced my hands onto the desk so as to stop them from shaking as his dark eyes jumped across the screen: the last photo in the album.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;I LIKE YOU

BOTTOM TEXT 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Manny shifted in his seat, stiff and small. “Oh,” he said, for the second time. (Maybe try using words instead of letters, Man. (I didn’t actually say that.)) I realized suddenly that I was smiling like a coyote and closed my lips. His cowlick bobbed, and he inspected the metal deskfeet, and then started to pick at his fingers. “Sorry,” he said next, which was at least a word. “I’m not, uh, I&amp;ndash;“ he finally met my eyes. “I don’t know if I really feel the same way.” His eyes slipped away in the middle of the word “same.” I knew exactly what &lt;em&gt;I&lt;/em&gt; felt in that moment. Manny, gathering his assignments and notebooks into a fabric binder, stumbled over the molded chairs and left the room.&lt;/p&gt;
&lt;p&gt;That was the last I ever saw of Manny. Well, basically. More accurately, that was that last I ever heard of him. Since that nightmarish block nine weeks ago, Manny has not said a word to me. I suppose he has made eye contact once, by accident. I made a particularly loud joke during a shared Bio 20 block, which caused his eyes to catch onto mine as our classmates crowded around the overhead projector. His eyes were still dark and full of light. I searched for hatred but found only detached curiosity, and perhaps tender pity. What a fucking joke.&lt;/p&gt;
&lt;p&gt;So, maybe that explains Manny to you. According to the field of psychology, repeated, short interactions between individuals can create a stronger sense of familiarity than intermittent deep conversations. I think this is what happened between us. Somehow, inside or between all of these interactions, I learned who Man was. So if you want to know something less personal, here are a couple more things. He loved the taste of MSG, but rarely ate a proper lunch. If I took the blue sleeve off his eraser, he would slip it back on without a word. He was timid, but those little movements were always swift and assured. Just like me, he is sixteen years old.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/rerereading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/rerereading/</guid>
      <description>&lt;h1 id=&#34;writing-that-is-still-not-meaningful&#34; &gt;Writing That is Still Not Meaningful?
&lt;span&gt;
    &lt;a href=&#34;#writing-that-is-still-not-meaningful&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;p&gt;December 2, 2025&lt;/p&gt;
&lt;span style=&#34;color:gray&#34;&gt;
&lt;i&gt;What exactly are we doing when we read LLM-generated text?&lt;/i&gt;
&lt;/span&gt;
&lt;p&gt;Recently, a &lt;a href=&#34;https://x.com/micahgoldblum/status/1989088547777966512?s=20&#34;&gt;nonsense LLM-generated paper&lt;/a&gt; kicked up some outrage in the AI community after receiving several positive reviews at ICLR, a large deep learning conference. Although it was flagged by a third reviewer and &lt;a href=&#34;https://x.com/iclr_conf/status/1989349884227715257?s=20&#34;&gt;later rejected for violating conference guidelines&lt;/a&gt;, I found it interesting that the fake paper had made it as far as it did. I was particularly moved by this reviewer&amp;rsquo;s comments:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/review3_big.png&#34; alt=&#34;I found this paper very difficult to read and comprehend. Beginning with the abstract—which conveys almost no meaningful insight to non-expert readers—the paper remains largely opaque throughout. It introduces numerous topics without proper context or explanation, ultimately leading to the unsubstantiated claim that a &amp;ldquo;verification framework&amp;rdquo; has been established.
There are two possible explanations for this lack of clarity: (1) The paper may be written in an extremely dense and narrow style, understandable only to experts working directly in this specific subarea (which I am not), or (2) The extensive use of LLM-assisted writing tools may have resulted in text that appears technically sophisticated but lacks genuine substance or coherence.&#34;&gt;&lt;/p&gt;
&lt;p&gt;The frustration here resonated with me a lot—probably because I&amp;rsquo;d been in a very similar situation before.&lt;/p&gt;
&lt;p&gt;In 2020, I close-read approximately 118 AI-generated documents on cannabis legalization (and an equal number of human-written ones). It was a painstaking task: I needed to classify the &lt;a href=&#34;https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23722/1/scidok_final.pdf&#34;&gt;lexical aspectual class&lt;/a&gt; of every clause, mark coherence relations between clauses, and rate the argumentation quality of each document. But there were two things that made this difficult. First, I didn&amp;rsquo;t know which articles were human-written and which were AI-generated. Second, the AI-generated documents were extremely uncanny. Take this sentence, for example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;tt&gt;If weed’s not really a public health issue and you&amp;rsquo;re really happy about it, get an understanding about the ways in which it will be able to influence your behaviour.&lt;/tt&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Is this someone&amp;rsquo;s Reddit comment, posted without a second thought? Or is it a semi-competent language model&amp;rsquo;s attempt to imitate the surface form of an argument? I was never really sure. But the quality of my annotations depended on actually understanding what was being said here, so I spent a lot of time re-reading these kinds of sentences over and over, trying to grasp some kind of meaning from them. It felt like I was having a stroke, or like I was being gaslit by the text. But then, I&amp;rsquo;d read something like&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;tt&gt;BART police already have a &amp;ldquo;marijuana alley&amp;rdquo; where potential customers could find sprayers and pagers ready to use and find it where they&amp;rsquo;re supposed to.&lt;/tt&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and breathe a sigh of relief. I&amp;rsquo;d know that this document is nonsense—that it was generated by GPT-2, a disembodied probabilistic model that cannot smoke weed and has never been to the Bay Area. Thus, I felt safe in assuming that there was &amp;ldquo;nothing there&amp;rdquo; for me to annotate.&lt;/p&gt;
&lt;p&gt;The following summer (2021), I wrote an &lt;a href=&#34;https://sfeucht.github.io/rereading&#34;&gt;essay&lt;/a&gt; about this experience for a class. It includes some cool examples of AI-generated nonsense that has the &lt;em&gt;shape&lt;/em&gt; of a sensible argument, without the content. In that essay, I argued that the text generated by LLMs is odd in that it is not language &lt;em&gt;yet&lt;/em&gt;—not until a human is able to read and extract meaning from that text. In this way, LLM-generated text is strangely beautiful.&lt;/p&gt;
&lt;h2 id=&#34;writing-thats-not-a-paper&#34; &gt;Writing That&amp;rsquo;s Not a Paper
&lt;span&gt;
    &lt;a href=&#34;#writing-thats-not-a-paper&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;&lt;p&gt;LLMs are a lot better now than they were in 2020: almost every sentence that comes out of a frontier model is not only structurally coherent, but sensical in the context of the sentences that came before it. You can almost always follow the flow of an LLM-generated article from beginning to end without much difficulty. So, does that mean that these documents are just&amp;hellip; meaningful now?&lt;/p&gt;
&lt;p&gt;Maybe not. There&amp;rsquo;s no fundamental difference between what GPT-2 does and what GPT-5 does (to our knowledge). If GPT-2 gave us &lt;em&gt;sentences&lt;/em&gt; that looked correct but did not actually say anything, then maybe GPT-5 gives us entire &lt;em&gt;papers&lt;/em&gt; that look coherent but don&amp;rsquo;t actually make any sense. Here&amp;rsquo;s a screenshot I took of the culprit paper&amp;rsquo;s introduction as an example.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/intro.png&#34; alt=&#34;Deploying large language models in sensitive settings creates a need for verifiable inference: proofs that outputs were computed correctly without revealing proprietary weights or private inputs. Zero-knowledge ML (ZKML) offers this, yet current systems struggle to scale to Transformer architectures.
Existing frameworks compile networks into polynomial constraint systems; prover cost is dominated by the number of constraints (roughly linear in parameter count). Cryptographic advances (e.g., lookups, sumcheck, commitments) accelerate the protocol layer, but they do not remove the model-level redundancy intrinsic to attention—leaving many constraints structurally unnecessary.
Key idea (GaugeZKP). Exploit attention&amp;rsquo;s gauge symmetries. Many parameterizations implement the same function. We rewrite deployed weights into a canonical form (constructed per head; see §3.4) without changing the model. A one-time Proof of Gauge Equivalence (PoGE) binds deployed and canonical weights; thereafter, per-inference Proofs of Verifiable Inference (PoVI) run only on the canonical model. Because this optimization is upstream of the prover, it composes with protocol-level speedups.
Scope and guarantees. We certify exact functional equivalence (no approximation); privacy follows from the zk proof system. Attention remains quadratic in sequence length. Canonicalization relies on full-column-rank projections and numerically stable QR/SPD roots; fixed-point precision uses scale 2^16 (deterministic choices yield identical outputs across implementations).&#34;&gt;&lt;/p&gt;
&lt;p&gt;I know nothing about this area, so I&amp;rsquo;ll leave critique of the &amp;ldquo;substance&amp;rdquo; of this paper to others (&lt;em&gt;if&lt;/em&gt; that substance exists; more on that later). But what struck me was that this introduction has all of the surface-level indicators of being cogent and well-written: plain language, italicized key terms, and even a bolded paragraph header that points you to the &lt;strong&gt;Key Idea.&lt;/strong&gt; Nonetheless, when I read it, I have no idea what problem the &amp;ldquo;authors&amp;rdquo; were trying to solve, or how their &amp;ldquo;key idea&amp;rdquo; actually helps to solve it.&lt;/p&gt;
&lt;p&gt;The way I feel reading this introduction is how I used to feel reading technical papers in undergrad, when I first started trying to get into ML research. Scanning over the text, it looks like something that should make a lot of sense to an expert somewhere, but no matter how many times I re-read any sentence, I am no closer to understanding what&amp;rsquo;s going on. It could be because I don&amp;rsquo;t know about ZKML, but I don&amp;rsquo;t think this is true. If I read the introduction of &lt;a href=&#34;https://arxiv.org/pdf/2404.16109&#34;&gt;this real paper&lt;/a&gt; on zero-knowledge proofs for LLMs, it&amp;rsquo;s like a breath of fresh air: I actually understand what ZKML is, why it&amp;rsquo;s interesting and hard, and even get a rough sense of what the authors did (even though it would take lots of work for me to truly understand it).&lt;/p&gt;
&lt;p&gt;Unfortunately, the folks who had to review this paper were subjected to worse LLM-gaslighting than I ever had to experience in my annotator days. In 2020, when I re-re-read a clause like &amp;ldquo;potential customers could find sprayers and pagers ready to use and find it where they&amp;rsquo;re supposed to,&amp;rdquo; the escape hatch was always right there (I knew it could be AI). But here, not only was the nonsensity of the text much more subtle, but the possibility that this paper was AI-generated might not have even crossed the reviewers&amp;rsquo; minds. If I was in this situation, I might have felt that old insecurity from undergrad creeping in, a nagging feeling that the problem is &lt;em&gt;me&lt;/em&gt;, maybe even that I should keep my head down and not object. I can see that being a factor for why this could get past so many reviewers.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-even-is-reading&#34; &gt;What Even Is Reading?
&lt;span&gt;
    &lt;a href=&#34;#what-even-is-reading&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;&lt;p&gt;This specific frustration of re-re-reading sentences to no avail isn&amp;rsquo;t new; I would guess that most people have experienced it with human-written text. I know that I can sometimes get &amp;ldquo;stuck&amp;rdquo; on sentences, reading them over and over like a broken record. For me, this mostly happens when reading technical writing, but I&amp;rsquo;ve also experienced it reading fiction, and even forum posts.&lt;/p&gt;
&lt;p&gt;Reading is usually very easy for us, and it almost happens involuntarily. Think of the Stroop effect, where it&amp;rsquo;s hard to &lt;em&gt;not&lt;/em&gt; read the content of a word when it&amp;rsquo;s flashed in front of you. When we read text that&amp;rsquo;s written by others, we understand their thoughts and intentions quite quickly, almost as if there&amp;rsquo;s a wire conducting their throughts straight into our brains. This might be why, at least in western English-speaking cultures, we conceptualize language as a conduit for meaning. This is known as the &lt;span style=&#34;font-variant:small-caps;&#34;&gt;Conduit Metaphor&lt;/span&gt;, &lt;a href=&#34;http://www.biolinguagem.com/ling_cog_cult/reddy_1979_conduit_metaphor.pdf&#34;&gt;first described&lt;/a&gt; by linguist Michael J. Reddy in 1979, who argued that it forms the basis of how English speakers conceptualize communication and meaning.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Under the &lt;span style=&#34;font-variant:small-caps;&#34;&gt;Conduit Metaphor&lt;/span&gt;, we think of thoughts/ideas as objects in our minds that can be &amp;ldquo;transferred&amp;rdquo; to other people. There&amp;rsquo;s two ideas going on here: first, that communication is the process of transferring thoughts to other people, and second, that language is the &lt;em&gt;container&lt;/em&gt; in which we put those thoughts (&amp;ldquo;put those ideas in some other paragraph,&amp;rdquo; &amp;ldquo;her words were filled with emotion&amp;rdquo;). If you study language, you&amp;rsquo;ve probably come across this assumption stated explicitly; for example, information-theoretic modeling of language conceptualizes communication as a noisy channel, where we encode meaning into messages that are then sent along this channel.&lt;/p&gt;
&lt;p&gt;However, as Reddy points out in his original essay, this metaphor is misleading. We can never actually access or experience the mind of another person, and ideas are not objects that we can take from our minds and wire directly into other people&amp;rsquo;s brains. If we could actually do that, we wouldn&amp;rsquo;t need language at all.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; Instead, communication is more like sending someone a recipe for a specific dish. Recipes do not inherently &amp;ldquo;contain&amp;rdquo; any food in them. Every person making a particular recipe will interpret the instructions differently, depending on their kitchen and the ingredients that they have access to in their own home, sometimes leading to very different outcomes. It would be nice if we could send our friends food (i.e., thoughts) directly, but because we don&amp;rsquo;t have a way to establish direct brain-to-brain contact, we have to make do with recipes (i.e., language).&lt;/p&gt;
&lt;!-- Instead of thinking of reading in terms of the &lt;span style=&#34;font-variant:small-caps;&#34;&gt;Conduit Metaphor&lt;/span&gt;, where a given text &#34;contains&#34; meaning that must be unpackaged by the reader, we could think of reading as a process of *generating* meaning from some object that exists in the world.  --&gt;
&lt;!-- [recipes do not inherently contain food in them.] Under this conceptualization, the process of reading is kind of like making a recipe from a cookbook—every person making a recipe will interpret the instructions differently, depending on their kitchen and the ingredients that they have access to in their own home, leading to very different outcomes. It would be nice if we could &#34;order takeout&#34; and directly transmit ideas into other people&#39;s brains, but since that is impossible, we have to make do with recipes/language.   --&gt;
&lt;!-- In this analogy where ideas are food, the &lt;span style=&#34;font-variant:small-caps;&#34;&gt;Conduit Metaphor&lt;/span&gt; would be akin to ordering UberEats (transporting someone else&#39;s food directly into your home). But since we can&#39;t actually directly transport ideas from one head to another, we have to make do with recipes, which we all interpret differently based on our own dietary restrictions and physical ingredients on hand.  --&gt;
&lt;!-- This captures the idea that when we read a text, we don&#39;t actually have access to the meaning [^3]  --&gt;
&lt;!--  --&gt;
&lt;p&gt;Under this &amp;ldquo;recipe&amp;rdquo; metaphor,&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; meaning is not inherently contained within a text, but &lt;em&gt;comes about&lt;/em&gt; as a result of a reader interpreting that text. I like this idea because it seems to resonate with other ways in which we use the word &amp;ldquo;read.&amp;rdquo; When we read tea leaves, we are generating meaning from a random blob based on our own personal symbols and conceptual structures. You can &lt;em&gt;read&lt;/em&gt; someone&amp;rsquo;s face, or &amp;ldquo;read into&amp;rdquo; their actions, but those interpretations will always be in terms of your own disposition, your own hopes and anxieties. The act of reading is always interpretation, rather than extraction: when someone says, &amp;ldquo;that&amp;rsquo;s my reading of it,&amp;rdquo; they are being totally precise, because every person will read a given text slightly differently.&lt;/p&gt;
&lt;p&gt;So what is happening when we find ourselves stuck re-re-reading the same sentence? If we accept that there is never any meaning &amp;ldquo;contained&amp;rdquo; inside a text, then these moments are not &lt;em&gt;failures&lt;/em&gt; to extract some true meaning lurking inside a work, but simply moments where some symbols on a page are not triggering any ideas in our minds. This could happen for any number of reasons: fatigue, failure on the author&amp;rsquo;s part to phrase their thoughts clearly, or simply a lack of relevant conceptual structure on the part of the reader (like when you try to read technical writing on an unfamiliar topic). And of course, it could happen when trying to read an artefact generated by a language model imitating the &lt;em&gt;form&lt;/em&gt; of a well-written argument, without any of the content. These moments show us that reading has always been a process of &lt;em&gt;generating&lt;/em&gt; meaning, not extracting it.&lt;/p&gt;
&lt;h2 id=&#34;ants-meaningfully-fornicating-in-the-sand&#34; &gt;Ants Meaningfully Fornicating In the Sand
&lt;span&gt;
    &lt;a href=&#34;#ants-meaningfully-fornicating-in-the-sand&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;&lt;p&gt;Under this framing, if a piece of text is meaningful to a reader, it doesn&amp;rsquo;t really matter &lt;em&gt;how&lt;/em&gt; that text came into being: whether it was human-written, AI-generated, or &lt;a href=&#34;https://arxiv.org/pdf/2308.05576&#34;&gt;formed by some ants in the dirt&lt;/a&gt;. This is starting to get into philosophical territory; since I don&amp;rsquo;t have a background in philosophy, I&amp;rsquo;ll just plainly state the issue that arises for me here.&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This &amp;ldquo;ants in the dirt&amp;rdquo; example that I linked to is from Hilary Putnam&amp;rsquo;s 1981 book, &lt;em&gt;Reason, Truth, and History.&lt;/em&gt; I learned about it from &lt;a href=&#34;https://arxiv.org/pdf/2308.05576&#34;&gt;Matthew Mandelkern and Tal Linzen&amp;rsquo;s 2024 article&lt;/a&gt;, &amp;ldquo;Do Language Models&amp;rsquo; Words Refer?&amp;rdquo;, who explain it very well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose that, at a picnic, you observe ants wending through the sand in a surprising pattern, which closely resembles the English sentence &amp;ldquo;Peano proved that arithmetic is incomplete&amp;rdquo;. At the same time, you get a text message from Luke, who is taking a logic class. He writes, &amp;ldquo;Peano proved that arithmetic is incomplete&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Intuitively, the two cases are very different, despite involving physically similar
patterns. The ants’ patterns do not say anything; they just happen to have formed
patterns which resemble meaningful words. Of course, you can interpret the pattern,
just as you can interpret an eagle’s flight as an auspicious augur; but these are interpretations you overlay on a natural pattern, not meanings intrinsic to the patterns themselves. By contrast, Luke’s words mean something definite on their own (regardless of whether you or anyone else interprets them): namely, that Peano proved that arithmetic is incomplete. What Luke said is false: it was Gödel who proved incompleteness. But
Luke said something, whereas the ants didn’t say anything at all.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- In particular, he said something false about Peano, which means that his (use of the) word ‘Peano’ managed
to refer to Peano. --&gt;
&lt;p&gt;If we apply the &amp;ldquo;recipe metaphor&amp;rdquo; to this scenario, then the meaning that arises in our minds when we read &amp;ldquo;Peano proved that arithmetic is incomplete&amp;rdquo; is the same for Luke&amp;rsquo;s text and for the ants in the dirt. In both scenarios, &amp;ldquo;Peano proved that arithmetic is incomplete&amp;rdquo; induces the exact same thought in our head (apart from non-linguistic concerns, like &amp;ldquo;why is Luke texting me this?&amp;rdquo; or &amp;ldquo;how the hell did these ants happen to spell out an entire sentence?&amp;rdquo;). But crucially, this meaning is &lt;em&gt;not&lt;/em&gt; &amp;ldquo;intrinsic to the patterns themselves&amp;rdquo;—it arises only when we see and interpret those patterns. And if someone else were to see the exact same patterns, the meaning that they would derive in their heads would be slightly different.&lt;/p&gt;
&lt;p&gt;This is what confuses me about debates on whether LLM-generated text is &amp;ldquo;truly meaningful.&amp;rdquo; If I read a sentence that causes me to have a particular thought, then I have made meaning from that sentence, and so the sentence is meaningful (to me). It used to be that LLM-generated text was hard to interpret, which meant that most of the time, it was not meaningful—unless you were a poor annotator like me, whose job it was to try very hard to interpret horrible things like &lt;a href=&#34;https://www.nytimes.com/2025/12/03/magazine/chatbot-writing-style.html&#34;&gt;&amp;ldquo;It’s all just the right amount of subtlety in male porn, and the amount of subtlety you can detect is simply astounding.&amp;quot;&lt;/a&gt; But now, LLM-generated text is so good that the majority of it is meaningful to most people almost all of the time.&lt;/p&gt;
&lt;p&gt;[the difference between luke and the ants maybe has to do with all of those metalinguistic considerations that go on when you read their sentences. knowing the context behind someone, and guessing their intentions, really helps with interpretation.] The question of communicative intent is different, though (and again, is where my lack of background is probably showing). [talk about what it means to write under this framework, trying to induce an idea in someone else&amp;hellip;]&lt;/p&gt;
&lt;p&gt;There are some infamous examples of fake papers making it into philosophy journals&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;even well-written human text. it&amp;rsquo;s not that we convert thoughts to words and words back to thoughts. it&amp;rsquo;s not a symmetric process. you can have thoughts you don&amp;rsquo;t write down, but more importantly, people can read anything out of your words that you weren&amp;rsquo;t thinking of. is this a continuum?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But&amp;hellip; what &lt;em&gt;if&lt;/em&gt; the thing that the reviewers read from the work was actually good?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fake paper examples that david mentioned. yeah like who cares if the author thought it was fake, if they did actually say something interesting&lt;/li&gt;
&lt;li&gt;derrida example?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a post-LLM world, we are now flooded with &amp;ldquo;meaningless&amp;rdquo; slop TODO talk about &amp;ldquo;faith&amp;rdquo; and generating meanings&amp;hellip;&lt;/p&gt;
&lt;!-- banging your head against a really difficult to understand paper. you can only do this if you have faith that the writer was a human who had something important they wanted to say.  --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Of course, the right thing to do in this situation would always be to ask for the paper to be reassigned, or review to the best of your ability with low confidence; if a paper is that hard to understand, it probably shouldn&amp;rsquo;t get published anyway. But a willingness to call BS seems to be a combination of seniority and personality.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;I learned about this idea from George Lakoff and Mark Johnson&amp;rsquo;s book, &lt;a href=&#34;https://en.wikipedia.org/wiki/Metaphors_We_Live_By&#34;&gt;&lt;em&gt;Metaphors We Live By&lt;/em&gt;&lt;/a&gt;, published in 1980. Thank you to Si Wu for recommending this book to me!&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Maybe the world would look something like the Human Instrumentality Project from Evangelion.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Reddy&amp;rsquo;s name for this is actually the &amp;ldquo;toolmakers paradigm,&amp;rdquo; but since the way I think about it might not match his original essay exactly, I won&amp;rsquo;t use his term in this post.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;If you have a philosophy background and are interested in talking about this, please reach out! This has been bothering me a lot.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/</guid>
      <description>&lt;style&gt;
    #papers {
        list-style: none;
        padding: 0;
        counter-reset: item;
    }
    
    .paper {
        /* background-color: #3a3a3a; */
        border: 1px solid #676767ff;
        padding: 15px 20px;
        margin-bottom: 20px;
        border-radius: 2px;
        counter-increment: item;
        position: relative;
    }
    
    .paper:before {
        font-weight: normal;
        /* color: #e0e0e0; */
        margin-right: 10px;
        font-size: 1.2em;
    }
    
    .paper h5 {
        display: inline;
        margin: 0;
        font-size: 1.2em;
        /* color: #ffffff; */
        font-weight: normal;
    }

    .paper span {
        display: inline;
        margin: 0;
        font-size: 0.8em;
        /* color: #ffffff; */
        font-weight: normal;
    }
    
    /* .paper:hover {
        background-color: #424242;
        transition: all 0.2s ease;
    } */
&lt;/style&gt;
&lt;h1&gt;Selected papers&lt;/h1&gt;
&lt;p&gt;See my &lt;a href=&#34;https://scholar.google.com/citations?user=4EobJQIAAAAJ&amp;hl=en&amp;oi=sra&#34;&gt;Google Scholar&lt;/a&gt; for a full list of publications.&lt;/p&gt;
&lt;ol id=&#34;papers&#34;&gt;
    &lt;li class=&#34;paper&#34;&gt;
    &lt;a href=&#34;https://dualroute.baulab.info/&#34;&gt;&lt;h5&gt;The Dual-Route Model of Induction&lt;/h5&gt;&lt;/a&gt;&lt;br&gt;
    &lt;span&gt;&lt;b&gt;Sheridan Feucht&lt;/b&gt;, Eric Todd, Byron Wallace, David Bau&lt;/span&gt;&lt;br&gt;
    &lt;span&gt;Second Conference on Language Modeling (COLM), 2025.&lt;/span&gt;
    &lt;/li&gt;
    &lt;li class=&#34;paper&#34;&gt;
        &lt;a href=&#34;https://footprints.baulab.info/&#34;&gt;&lt;h5&gt; Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs&lt;/h5&gt;&lt;/a&gt;&lt;br&gt;
        &lt;span&gt;&lt;b&gt;Sheridan Feucht&lt;/b&gt;, David Atkinson, Byron Wallace, David Bau&lt;/span&gt;&lt;br&gt;
        &lt;span&gt;Empirical Methods in Natural Language Processing (EMNLP), 2024.&lt;/span&gt;
    &lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Other Works&lt;/h1&gt;
&lt;ol id=&#34;papers&#34;&gt;
    &lt;li class=&#34;paper&#34;&gt;
        &lt;a href=&#34;https://arithmetic.baulab.info/&#34;&gt;&lt;h5&gt;Vector Arithmetic in Concept and Token Subspaces&lt;/h5&gt;&lt;/a&gt;&lt;br&gt;
        &lt;span&gt;&lt;b&gt;Sheridan Feucht&lt;/b&gt;, Byron Wallace, David Bau&lt;/span&gt;&lt;br&gt;
        &lt;span&gt;Mechanistic Interpretability Workshop at NeurIPS, 2025.&lt;/span&gt;
    &lt;/li&gt;
    &lt;li class=&#34;paper&#34;&gt;
        &lt;a href=&#34;https://elm.baulab.info/&#34;&gt;&lt;h5&gt;Erasing Conceptual Knowledge from Language Models&lt;/h5&gt;&lt;/a&gt;&lt;br&gt;
        &lt;span&gt;Rohit Gandikota, &lt;b&gt;Sheridan Feucht&lt;/b&gt;, Samuel Marks, David Bau&lt;/span&gt;&lt;br&gt;
        &lt;span&gt;Conference on Neural Information Processing Systems (NeurIPS), 2025.&lt;/span&gt;
    &lt;/li&gt;
    &lt;li class=&#34;paper&#34;&gt;
        &lt;a href=&#34;https://arxiv.org/abs/2310.09612&#34;&gt;&lt;h5&gt;Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations&lt;/h5&gt;&lt;/a&gt;&lt;br&gt;
        &lt;span&gt;Alexa R. Tartaglini*, &lt;b&gt;Sheridan Feucht*&lt;/b&gt;, Michael A. Lepori, Wai Keen Vong, Charles Lovering, Brenden M. Lake, Ellie Pavlick&lt;/span&gt;&lt;br&gt;
        &lt;span&gt;Conference on Cognitive Computational Neuroscience, 2025.&lt;/span&gt;
    &lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/syllogisms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/syllogisms/</guid>
      <description>&lt;h1 id=&#34;solving-syllogisms-is-not-intelligence&#34; &gt;Solving Syllogisms is Not Intelligence
&lt;span&gt;
    &lt;a href=&#34;#solving-syllogisms-is-not-intelligence&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h1&gt;&lt;p&gt;April 23, 2025&lt;/p&gt;
&lt;span style=&#34;color:gray&#34;&gt;
&lt;i&gt;I think that we overvalue logical reasoning when it comes to measuring &#34;intelligence.&#34;&lt;/i&gt;
&lt;/span&gt;
&lt;p&gt;What do we mean by &lt;em&gt;intelligence&lt;/em&gt; in the context of cognitive science and AI? One thing that often comes to mind is logical reasoning: deriving conclusions from a set of axioms and rules, or thinking through syllogisms like &lt;em&gt;&amp;ldquo;all cats are mammals, all mammals are warm-blooded, therefore all cats are warm-blooded.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Until recently, I thought of structured reasoning tasks as central to human (and by extension machine) intelligence. It seemed almost obvious that a &amp;ldquo;truly&amp;rdquo; intelligent system would encode abstract notions of, e.g., &lt;a href=&#34;https://arxiv.org/abs/2310.09612&#34;&gt;same versus different,&lt;/a&gt; and that such a system would have absolutely no trouble with syllogistic tasks. I don&amp;rsquo;t think this is an unpopular opinion: critics of LLMs assert that logical tasks are &lt;a href=&#34;https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and&#34;&gt;essential to &amp;ldquo;true&amp;rdquo; general intelligence&lt;/a&gt;. Some go on to claim that this type of reasoning is fundamentally incompatible with probablistic next-token prediction.&lt;/p&gt;
&lt;p&gt;But I&amp;rsquo;m not sure that abstract logical reasoning is quite as fundamental to human intelligence as it seems. In his 1982 work &lt;em&gt;Orality and Literacy&lt;/em&gt;, Walter Ong references a &lt;a href=&#34;https://dl1.cuni.cz/pluginfile.php/738180/mod_resource/content/0/Luria%20-%20Cognitive-development-its-cultural-and-social-foundations.pdf&#34;&gt;book containing a series of interviews&lt;/a&gt; by A.R. Luria, a Soviet neuropsychologist who interviewed low-educated populations of peasants in the Uzbek SSR in 1931. The interviews were published over forty years later, in 1976, and are very interesting to me.&lt;/p&gt;
&lt;p&gt;Here is one of the questions Luria asked people:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In the Far North, where there is snow, all bears are white. Novaya Zemlya is in the far north and there is always snow there. What color are the bears?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Strikingly, Luria found that illiterate respondents were very unlikely to simply respond &amp;ldquo;white&amp;rdquo; to this question. They ignored the constraints of the syllogism, and reasoned based on their own personal experiences instead. Here is an example of an exchange with a 37-year-old from a remote Kashgar village &lt;a href=&#34;https://dl1.cuni.cz/pluginfile.php/738180/mod_resource/content/0/Luria%20-%20Cognitive-development-its-cultural-and-social-foundations.pdf&#34;&gt;(Luria, 1976, pp. 109)&lt;/a&gt;, who refused to accept the premise of the question.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In the Far North, where there is snow, all bears are white. Novaya Zemlya is in the far north and there is always snow there. What color are the bears?&lt;/strong&gt;&lt;br&gt;
&amp;ldquo;There are different sorts of bears.&amp;rdquo; &lt;br&gt;
&lt;strong&gt;(The syllogism is repeated.)&lt;/strong&gt; &lt;br&gt;
&amp;ldquo;I don&amp;rsquo;t know. I&amp;rsquo;ve seen a black bear. I&amp;rsquo;ve never seen any others&amp;hellip; each locality has its own animals: if it&amp;rsquo;s white, they will be white; if it&amp;rsquo;s yellow, they will be yellow.&amp;rdquo; &lt;br&gt;
&lt;strong&gt;But what kind of bears are there in Novaya Zemlya?&lt;/strong&gt;&lt;br&gt;
&amp;ldquo;We always speak only of what we see, we don&amp;rsquo;t talk about what we haven&amp;rsquo;t seen.&amp;quot;&lt;br&gt;
&lt;strong&gt;But what do my words imply? (The syllogism is repeated.)&lt;/strong&gt;&lt;br&gt;
&amp;ldquo;Well, it&amp;rsquo;s like this: our tsar isn&amp;rsquo;t like yours, and yours isn&amp;rsquo;t like ours. Your words can be answered only by someone who was there, and if a person wasn&amp;rsquo;t there he can&amp;rsquo;t say anything on the basis of your words.&amp;rdquo; &lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Luria presents illiterate Uzbeks with many more logical puzzles, which they also seem to reject. When shown drawings of an axe, a saw, a hatchet, and a log, and asked to choose the odd one out, a 25-year-old illiterate respondent said: &amp;ldquo;They&amp;rsquo;re all alike. The saw will saw the log and the hatchet will chop it into small pieces. If one of these has to go, I&amp;rsquo;d throw out the hatchet. It doesn&amp;rsquo;t do as good a job as a saw.&amp;rdquo; Literate respondents, on the other hand, would almost always discard the log, with the rationale that the three tools should be grouped together.&lt;/p&gt;
&lt;p&gt;Regardless of the underlying explanation for this behavior, I find these interview responses thrilling to read. There are a lot of hypotheses for why these people were so resistant to such logic-based tasks (e.g., Ong points out that people who had learned to write were more likely to entertain Luria&amp;rsquo;s syllogisms), but there is no reason to believe that regular people born in the Uzbek SSR and randomly selected for this study were any more or less &amp;ldquo;intelligent&amp;rdquo; than individuals drawn from some other population. Clearly, abstract reasoning tasks like deduction and categorization are &lt;em&gt;not&lt;/em&gt; universal across humans—and these Uzbek farmers were getting along just fine without much care for them!&lt;/p&gt;
&lt;h3 id=&#34;some-sympathy&#34; &gt;Some Sympathy
&lt;span&gt;
    &lt;a href=&#34;#some-sympathy&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h3&gt;&lt;p&gt;To go back to the bear example, it&amp;rsquo;s not that people &lt;em&gt;couldn&amp;rsquo;t conceive&lt;/em&gt; of the abstract reasoning necessary to answer this problem &amp;ldquo;correctly.&amp;rdquo; After more back-and-forth in the above conversation, another young Uzbek chimed in, who was clearly able to complete the syllogistic reasoning presented in the question.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;But on the basis of my words—in the North, where there is always snow, the bears are white, can you gather what kind of bears there are in Novaya Zemlya?&lt;/strong&gt;&lt;br&gt;
&amp;ldquo;If a man was sixty or eighty and had seen a white bear and had told about it, he could be believed, but I&amp;rsquo;ve never seen one and hence I can&amp;rsquo;t say. That&amp;rsquo;s my last word. Those who saw can tell, and those who didn&amp;rsquo;t see can&amp;rsquo;t say anything!&amp;rdquo; (At this point a young Uzbek volunteered, &amp;ldquo;From your words it means that bears there are white.&amp;rdquo;)&lt;br&gt;
&lt;strong&gt;Well, which of you is right?&lt;/strong&gt;&lt;br&gt;
&amp;ldquo;What the cock knows how to do, he does. What I know, I say, and nothing beyond that!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reading the transcripts of these interviews, it seems to me that these questions are just frustratingly uninteresting to the interviewed subjects, who were not used to contrived tests of formal reasoning in classroom settings. It&amp;rsquo;s not unreasonable to respond this way if you aren&amp;rsquo;t used to separating a word problem from the world that it is embedded in.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example that might make you more sympathetic to the Uzbek nomads. What valid inference can you make based on the following statements (&lt;a href=&#34;https://modeltheory.org/papers/1989suppression.pdf&#34;&gt;Byrne, 1989&lt;/a&gt;)?&lt;/p&gt;
&lt;!-- http://repo.darmajaya.ac.id/4379/1/Computational%20logic%20and%20human%20thinking%20_%20how%20to%20be%20artificially%20intelligent%20%28%20PDFDrive%20%29.pdf
https://modeltheory.org/papers/1989suppression.pdf --&gt;
&lt;blockquote&gt;
&lt;p&gt;If Amy has an essay to write, she will study late in the library.
&lt;br&gt; If the library is open, Amy will study late in the library. &lt;br&gt; Amy has an essay to write.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here you are supposed to reason logically within the confines of these three sentences, which translated into symbols, say: $P\rightarrow Q$, $M\rightarrow Q$, $P$. Operating logically, I know that I can conclude $Q$. But personally, I find it very hard to conclude that &amp;ldquo;Amy will study late in the library&amp;rdquo; without knowing whether the library is open. There is an obvious &lt;em&gt;modus ponens&lt;/em&gt; here, but I just cannot make myself believe it. Others agree: 62% of respondents in Byrne&amp;rsquo;s original study made the same &amp;ldquo;error&amp;rdquo; that I did.&lt;/p&gt;
&lt;p&gt;So—if you agree with me that we can&amp;rsquo;t conclude whether Amy will study late unless we know the closing hours of the library she plans to study at, then maybe you can understand the indignation of the 37-year-old Uzbek when asked whether bears in Novaya Zemlya are white. When situated in a real life scenario, it just doesn&amp;rsquo;t make sense to reason in this way. Suspending all external world knowledge and understanding to complete a word problem just feels inane. We are used to suspending our disbelief when it comes to classic syllogisms, but once the reasoning hits a little closer to home, a lot of us end up reacting just like Luria&amp;rsquo;s interviewees.&lt;/p&gt;
&lt;h2 id=&#34;quick-conclusion&#34; &gt;Quick Conclusion
&lt;span&gt;
    &lt;a href=&#34;#quick-conclusion&#34;&gt;
        &lt;svg viewBox=&#34;0 0 28 23&#34; height=&#34;100%&#34; width=&#34;19&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;&lt;path d=&#34;M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;path d=&#34;M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71&#34; fill=&#34;none&#34; stroke-linecap=&#34;round&#34; stroke-miterlimit=&#34;10&#34; stroke-width=&#34;2&#34;/&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/span&gt;
&lt;/h2&gt;&lt;p&gt;So, is the skill of suspending disbelief in order to activate particular logical processes, given a particular type of question, a good indicator of general intelligence? Maybe our focus on &amp;ldquo;intelligence&amp;rdquo; is myopic in and of itself. In &lt;em&gt;Orality and Literacy&lt;/em&gt;, Ong notes that people from many oral cultures do not think of individuals as being &amp;ldquo;intelligent&amp;rdquo; in general. If someone is a good navigator, they are a good navigator. If they are a dancer or storyteller, they are lauded for those abilities. Why would you need some underlying g-factor to tie all of these traits together? And in the case of IQ testing, why would shape rotation and syllogism questions be uniquely suited to measure that factor, if it even exists? Even ignoring the &lt;a href=&#34;https://en.wikipedia.org/wiki/Intelligence_quotient#IQ_testing_and_the_eugenics_movement_in_the_United_States&#34;&gt;dark history of IQ testing&lt;/a&gt;, I think that this perspective is rigid and unhelpful. There is so much more to the human mind beyond formal logic and sterile test questions; arguably, the beauty of artificial and natural thought comes from its inconsistencies, illogical leaps, and unconscious intuitions.&lt;/p&gt;
&lt;!-- Ironically, I&#39;m now suspicious of the word &#34;intelligence&#34; in AI research when it refers to these narrow abstract reasoning tasks. Not just because of pedantic nitpicking, not just because of the , but because I simply think it&#39;s a really narrow view of the human mind, which makes it unhelpful for our purposes as AI researchers. Maybe we should think bigger about what exactly we mean by *intelligence* other than pointing to these types of structured tasks, especially if our purported goal is to build intelligent systems.  --&gt;
&lt;!-- 
The word problems in this anecdote are clearly reminiscent of how we measure IQ in intelligence testing. One conclusion that you might draw from this is that if illiterate people are less good at  --&gt;
</description>
    </item>
    
  </channel>
</rss>
